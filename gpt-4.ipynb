{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial\n",
    "import pretty_errors\n",
    "import timeit\n",
    "import logging\n",
    "import time\n",
    "import asyncio\n",
    "from openai.error import OpenAIError\n",
    "import json\n",
    "from typing import Callable\n",
    "from utils.preprocess import individual_preprocess\n",
    "from dotenv import load_dotenv\n",
    "from utils.prompts import *\n",
    "from utils.SummariseJob import summarise_job_gpt\n",
    "from utils.AsyncSummariseJob import async_summarise_description\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from utils.handy import e5_base_v2_query, filter_last_two_weeks, append_parquet\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    stop_after_attempt,\n",
    "    wait_random_exponential,\n",
    ")  # for exponential backoff\n",
    "\n",
    "\n",
    "load_dotenv('.env')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "user = os.getenv(\"user\")\n",
    "password = os.getenv(\"password\")\n",
    "host = os.getenv(\"host\")\n",
    "port = os.getenv(\"port\")\n",
    "database = os.getenv(\"database\")\n",
    "SAVE_PATH = os.getenv(\"SAVE_PATH\")\n",
    "E5_BASE_V2_DATA = os.getenv(\"E5_BASE_V2_DATA\")\n",
    "\n",
    "\n",
    "#Start the timer\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "#GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "GPT_MODEL = \"gpt-4\"\n",
    "#GPT_MODEL = \"gpt-3.5-turbo-16k\"\n",
    "\"\"\"\"\n",
    "Load the embedded file\n",
    "\"\"\"\n",
    "\n",
    "logging.basicConfig(filename='/Users/juanreyesgarcia/Library/CloudStorage/OneDrive-FundacionUniversidaddelasAmericasPuebla/DEVELOPER/PROJECTS/DreamedJobAI/logs/LoggingGPT4.log', level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings_path = E5_BASE_V2_DATA\n",
    "\n",
    "df_unfiltered = pd.read_parquet(embeddings_path)\n",
    "\n",
    "df = filter_last_two_weeks(df_unfiltered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ids_ranked_by_relatedness_e5(query: str,\n",
    "    df: pd.DataFrame,\n",
    "    min_n: int,\n",
    "    top_n: int,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \n",
    "    #the query is embedded using e5\n",
    "    query_embedding = e5_base_v2_query(query=query)\n",
    "\n",
    "    ids_and_relatednesses = [\n",
    "        (row[\"id\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    ids_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    ids, relatednesses = zip(*ids_and_relatednesses)\n",
    "    return ids[min_n:top_n], relatednesses[min_n:top_n]     \n",
    "    #Returns a list of strings and relatednesses, sorted from most related to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nids, relatednesses = ids_ranked_by_relatedness_e5(abstract_cv, df, min_n=0, top_n=20)\\nfor id, relatedness in zip(ids, relatednesses):\\n    logging.info(f\"ID: {id} has the following {relatedness=:.3f}\")'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "ids, relatednesses = ids_ranked_by_relatedness_e5(abstract_cv, df, min_n=0, top_n=20)\n",
    "for id, relatedness in zip(ids, relatednesses):\n",
    "    logging.info(f\"ID: {id} has the following {relatedness=:.3f}\")\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiktoken function -> to count tokens\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_query_summary(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int,\n",
    "    min_n: int,\n",
    "    top_n: int\n",
    ") -> str:\n",
    "    #Return a message for GPT, with relevant source texts pulled from a dataframe.\n",
    "    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df, min_n=min_n, top_n=top_n)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = introduction_prompt\n",
    "    query_user = f\"{query}\"\n",
    "    message = introduction\n",
    "    # Create a list of tasks\n",
    "    tasks = [async_summarise_description(df[df['id'] == id]['description'].values[0]) for id in ids]\n",
    "\n",
    "    # Run the tasks concurrently\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    job_summaries = []\n",
    "    total_cost_summaries = 0    \n",
    "\n",
    "    for id, result in zip(ids, results):\n",
    "        job_description_summary, cost, elapsed_time = result\n",
    "        \n",
    "        # Append summary to the list\n",
    "        job_summaries.append({\n",
    "            \"id\": id,\n",
    "            \"summary\": job_description_summary\n",
    "        })\n",
    "        #Append total cost\n",
    "        total_cost_summaries += cost\n",
    "\n",
    "        next_id = f'\\nID:<{id}>\\nJob Description:---{job_description_summary}---\\n'\n",
    "        if (\n",
    "            num_tokens(message + next_id + query_user, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return query_user, message, job_summaries, total_cost_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\n",
    "async def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    min_n: int,\n",
    "    top_n: int,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 8192,\n",
    "    log_gpt_messages: bool = True,\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    query_user, job_id_description, job_summaries, total_cost_summaries = await async_query_summary(query, df, model=model, token_budget=token_budget, min_n=min_n, top_n=top_n)\n",
    "\n",
    "    #Save summaries in a df & then parquet -> append data if function called more than once\n",
    "    df_summaries = pd.DataFrame(job_summaries)\n",
    "    #logging.info(df_summaries)\n",
    "    #df_summaries.to_parquet(SAVE_PATH+ f'/summaries.parquet', engine='pyarrow')\n",
    "\n",
    "    append_parquet(df_summaries, 'summaries')\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\n",
    "        {\"role\": \"assistant\", \"content\": job_id_description}\n",
    "    ]\n",
    "    \n",
    "    if log_gpt_messages:\n",
    "        logging.info(messages)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    #if print_cost_and_relatednesses:\n",
    "    total_tokens = response['usage']['total_tokens']\n",
    "    prompt_tokens = response['usage']['prompt_tokens']\n",
    "    completion_tokens = response['usage']['completion_tokens']\n",
    "    logging.info(f\"\\nOPERATION: GPT-3.5 TURBO SUMMARISING. \\nTOTAL COST: ${total_cost_summaries} USD\")\n",
    "    #logging.info(f\"OPERATION: {GPT_MODEL} CLASSIFYING \\nPROMPT TOKENS USED:{prompt_tokens}\\n COMPLETION TOKENS USED:{completion_tokens}\\n \\nTOTAL TOKENS USED:{total_tokens}\\n)\n",
    "\n",
    "    #Approximate cost\n",
    "    if GPT_MODEL == \"gpt-4\":\n",
    "        prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\n",
    "        completion_cost = round((completion_tokens / 1000) * 0.06, 3)\n",
    "        cost_classify = prompt_cost + completion_cost\n",
    "        logging.info(f\"\\nOPERATION: {GPT_MODEL} CLASSIFICATION \\nPROMPT TOKENS USED:{prompt_tokens}\\nCOMPLETION TOKENS USED:{completion_tokens}\\nTOTAL TOKENS USED:{total_tokens}\\nCOST FOR CLASSIFYING: ${cost_classify} USD\")\n",
    "    elif GPT_MODEL == \"gpt-3.5-turbo\":\n",
    "        prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\n",
    "        completion_cost = round((completion_tokens / 1000) * 0.002, 3)\n",
    "        cost_classify = prompt_cost + completion_cost\n",
    "        logging.info(f\"\\nOPERATION: {GPT_MODEL} CLASSIFICATION \\nPROMPT TOKENS USED:{prompt_tokens}\\nCOMPLETION TOKENS USED:{completion_tokens}\\nTOTAL TOKENS USED:{total_tokens}\\nCOST FOR CLASSIFYING: ${cost_classify} USD\")\n",
    "    elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\n",
    "        prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\n",
    "        completion_cost = round((completion_tokens / 1000) * 0.004, 3)\n",
    "        cost_classify = prompt_cost + completion_cost\n",
    "        logging.info(f\"\\nOPERATION: {GPT_MODEL} CLASSIFICATION \\nPROMPT TOKENS USED:{prompt_tokens}\\nCOMPLETION TOKENS USED:{completion_tokens}\\nTOTAL TOKENS USED:{total_tokens}\\nCOST FOR CLASSIFYING: ${cost_classify} USD\")\n",
    "\n",
    "    #relatednesses\n",
    "    ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df, min_n=min_n, top_n=top_n)\n",
    "    for id, relatedness in zip(ids, relatednesses):\n",
    "        logging.info(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "    \n",
    "    elapsed_time = (timeit.default_timer() - start_time) / 60\n",
    "    logging.info(f\"\\nGPT-3.5 TURBO & GPT-4 finished summarising and classifying! all in: {elapsed_time:.2f} minutes \\n\")\n",
    "    \n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def check_output_GPT4(input_cv: str, min_n:int, top_n:int) -> str:\n",
    "    default = '[{\"id\": \"\", \"suitability\": \"\", \"explanation\": \"\"}]'\n",
    "    default_json = json.loads(default)\n",
    "    \n",
    "    for _ in range(6):\n",
    "        i = _ + 1\n",
    "        try:\n",
    "            python_string = await ask(query=input_cv, min_n=min_n, top_n=top_n)\n",
    "            try:\n",
    "                data = json.loads(python_string)\n",
    "                logging.info(f\"Response is a valid json object. Done in loop number: {i}\")\n",
    "                return data\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        except OpenAIError as e:\n",
    "            logging.warning(f\"{e}. Retrying in 10 seconds. Number of retries: {i}\")\n",
    "            time.sleep(10)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            logging.warning(f\"{e}. Retrying in 5 seconds. Number of retries: {i}\")\n",
    "            time.sleep(5)\n",
    "            pass\n",
    "\n",
    "    logging.error(\"Check logs!!!! Main function was not callable. Setting json to default\")\n",
    "    return default_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_dataframe_display_options():\n",
    "    pd.set_option('display.max_columns', None)  # Show all columns\n",
    "    pd.set_option('display.max_rows', None)  # Show all rows\n",
    "    pd.set_option('display.width', None)  # Disable column width restriction\n",
    "    pd.set_option('display.expand_frame_repr', False)  # Disable wrapping to multiple lines\n",
    "    pd.set_option('display.max_colwidth', None)  # Display full contents of each column\n",
    "\n",
    "# Call the function to set the desired display options\n",
    "set_dataframe_display_options()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def main():\n",
    "    min_n=0\n",
    "    top_n=10\n",
    "\n",
    "    # Define the suitable categories\n",
    "    suitable_categories = ['Highly Suitable', 'Moderately Suitable', 'Potentially Suitable']\n",
    "\n",
    "    # Initialize the dataframe\n",
    "    df_appended = pd.DataFrame()\n",
    "\n",
    "    # Continue to call the function until we have 10 suitable jobs\n",
    "    counter = 0\n",
    "    while True:\n",
    "        checked_json = await check_output_GPT4(input_cv=abstract_cv, min_n=min_n, top_n=top_n)\n",
    "        \n",
    "        # Convert the JSON to a dataframe and append it to the existing dataframe\n",
    "        df_original = pd.read_json(json.dumps(checked_json))\n",
    "        df_appended = pd.concat([df_appended, df_original], ignore_index=True)\n",
    "        \n",
    "        counter += 1\n",
    "        logging.info(f\"Looking for suitable jobs. Current loop: {counter}\")\n",
    "\n",
    "        logging.info(f\"Current min_n: {min_n}. Current top_n: {top_n}\")\n",
    "\n",
    "        # Increment the counters\n",
    "        min_n += 10\n",
    "        top_n += 10\n",
    "\n",
    "        # Filter the dataframe to only include the suitable jobs\n",
    "        df_most_suitable = df_appended[df_appended['suitability'].isin(suitable_categories)] if 'suitability' in df_appended.columns else pd.DataFrame()\n",
    "        \n",
    "        df_appended.to_parquet(SAVE_PATH + f\"/df_appended.parquet\", index=False)\n",
    "        df_most_suitable.to_parquet(SAVE_PATH + f\"/df_most_suitable.parquet\", index=False)\n",
    "\n",
    "        # Break the loop if we have 10 suitable jobs\n",
    "        if len(df_most_suitable) >= 10:\n",
    "            break\n",
    "\n",
    "    logging.info(f\"\\nDF APPENDED:\\n{df_appended}\")\n",
    "    \n",
    "    #Get the ids\n",
    "    def ids_df_most_suitable(df: pd.DataFrame = df_most_suitable) -> str:\n",
    "        ids = \"\"\n",
    "        for _, row in df.iterrows():\n",
    "            if \"id\" in row:\n",
    "                if ids:\n",
    "                    ids += \", \"\n",
    "                ids += f\"'{row['id']}'\"\n",
    "\n",
    "        return f\"({ids})\"\n",
    "\n",
    "    ids_most_suitable = ids_df_most_suitable()\n",
    "    logging.info(f\"Getting the ids from the json object: {type(ids_most_suitable)}, {ids_most_suitable}\")\n",
    "\n",
    "    def find_jobs_per_ids(ids:str, table: str = \"main_jobs\") -> pd.DataFrame:\n",
    "        conn = psycopg2.connect(user=user, password=password, host=host, port=port, database=database)\n",
    "        # Create a cursor object\n",
    "        cur = conn.cursor()\n",
    "        #TABLE SHOULD EITHER BE \"main_jobs\" or \"test\"\n",
    "        cur.execute( f\"SELECT id, title, link, location FROM {table} WHERE id IN {ids}\")\n",
    "\n",
    "        # Fetch all rows from the table\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        # Separate the columns into individual lists\n",
    "        all_ids = [row[0] for row in rows]\n",
    "        all_titles = [row[1] for row in rows]\n",
    "        all_links = [row[2] for row in rows]\n",
    "        all_locations = [row[3] for row in rows]\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'id': all_ids,\n",
    "            'title': all_titles,\n",
    "            'link': all_links,\n",
    "            'location': all_locations\n",
    "        })\n",
    "                # Close the database connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    df_postgre = find_jobs_per_ids(ids=ids_most_suitable)\n",
    "\n",
    "    #Read the parquet with ids & summaries\n",
    "    df_summaries = pd.read_parquet(SAVE_PATH + \"/summaries.parquet\")\n",
    "    #Merge it with the data in postgre\n",
    "    df_postgre_summaries = df_postgre.merge(df_summaries, on='id', how='inner')\n",
    "    #Merge with most suitable df so you have all the rows\n",
    "    df = df_postgre_summaries.merge(df_most_suitable, on=\"id\", how='inner')\n",
    "\n",
    "    logging.info(f\"\\nALL ROWS:\\n{df}\")\n",
    "\n",
    "\n",
    "    def sort_df_by_suitability(df: pd.DataFrame = df) -> pd.DataFrame:\n",
    "        custom_order = {\n",
    "            'Highly Suitable': 1,\n",
    "            'Moderately Suitable': 2,\n",
    "            'Potentially Suitable': 3\n",
    "        }\n",
    "        df['suitability_rank'] = df['suitability'].map(custom_order)\n",
    "        sorted_df = df.sort_values(by='suitability_rank')\n",
    "        sorted_df = sorted_df.drop(columns='suitability_rank')\n",
    "        return sorted_df\n",
    "\n",
    "    sorted_df = sort_df_by_suitability()\n",
    "\n",
    "    filename = \"/final_user_df\"\n",
    "    \n",
    "    sorted_df.to_parquet(SAVE_PATH + f\"{filename}.parquet\", index=False)\n",
    "\n",
    "    logging.info(f\"\\nSORTED DF:\\n{sorted_df}.\\n\\nThis df has been saved in ...{filename}.parquet\\n\\n\\n\")\n",
    "\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nasync def main():\\n\\n    #TODO: This needs to be user\\'s cv\\n    #checked_json = await check_output_GPT4(input_cv=abstract_cv, min_n=0, top_n=10)\\n\\n    #Try block needs to be here\\n\\n    def ids_json_loads(data: list[dict[str, str, str]] = None) -> str:\\n        if data is None:\\n            data = checked_json\\n            logging.info(f\"type of the json object: {type(data)} Data: {data}\")\\n            #print(type(exp), exp)\\n        \\n        ids = \"\"\\n        for item in data:\\n            if \"id\" in item:\\n                if ids:\\n                    ids += \", \"\\n                ids += f\"\\'{item[\\'id\\']}\\'\"\\n\\n        return f\"({ids})\"\\n\\n    ids_ready = ids_json_loads()\\n    logging.info(f\"Getting the ids from the json object: {type(ids_ready)}, {ids_ready}\")\\n\\n    def ids_json_loads(df: pd.DataFrame = df_most_suitable) -> str:\\n\\n        ids = \"\"\\n        for _, row in df.iterrows():\\n            if \"id\" in row:\\n                if ids:\\n                    ids += \", \"\\n                ids += f\"\\'{row[\\'id\\']}\\'\"\\n\\n        return f\"({ids})\"\\n\\n    ids_ready = ids_json_loads()\\n    logging.info(f\"Getting the ids from the json object: {type(ids_ready)}, {ids_ready}\")\\n\\n\\n    def find_jobs_per_ids(ids:str, table: str = \"test\") -> pd.DataFrame:\\n        conn = psycopg2.connect(user=user, password=password, host=host, port=port, database=database)\\n        # Create a cursor object\\n        cur = conn.cursor()\\n        #TABLE SHOULD EITHER BE \"main_jobs\" or \"test\"\\n        cur.execute( f\"SELECT id, title, link, location FROM {table} WHERE id IN {ids}\")\\n\\n        # Fetch all rows from the table\\n        rows = cur.fetchall()\\n\\n        # Separate the columns into individual lists\\n        all_ids = [row[0] for row in rows]\\n        all_titles = [row[1] for row in rows]\\n        all_links = [row[2] for row in rows]\\n        all_locations = [row[3] for row in rows]\\n\\n        df = pd.DataFrame({\\n            \\'id\\': all_ids,\\n            \\'title\\': all_titles,\\n            \\'link\\': all_links,\\n            \\'location\\': all_locations\\n        })\\n\\n\\n        # Close the database connection\\n        cur.close()\\n        conn.close()\\n\\n        return df\\n\\n    df_postgre = find_jobs_per_ids(ids=ids_ready)\\n    #Read the parquet\\n    df_summaries = pd.read_parquet(SAVE_PATH + \"/summaries.parquet\")\\n\\n    df = df_postgre.merge(df_summaries, on=\\'id\\', how=\\'inner\\')\\n\\n    logging.info(f\"RELATED JOBS & THEIR SUMMARIES: \\n {df}\")\\n\\n    def adding_all_data(df: pd.DataFrame, suitable_jobs: list) -> pd.DataFrame:\\n        for index, row in df.iterrows():\\n            entry_id = row[\\'id\\']\\n            for json_item in suitable_jobs:\\n                if int(json_item[\\'id\\']) == entry_id:\\n                    suitability = json_item[\\'suitability\\']\\n                    explanation = json_item[\\'explanation\\']\\n                    df.at[index, \\'suitability\\'] = suitability\\n                    df.at[index, \\'explanation\\'] = explanation\\n                    break\\n        return df\\n\\n    updated_data = adding_all_data(df=df, suitable_jobs=checked_json)\\n\\n    logging.info(f\"ALL COLUMNS: \\n {updated_data}\")\\n\\n    def sort_df_by_suitability(df: pd.DataFrame = df) -> pd.DataFrame:\\n        custom_order = {\\n            \\'Highly Suitable\\': 1,\\n            \\'Moderately Suitable\\': 2,\\n            \\'Potentially Suitable\\': 3,\\n            \\'Marginally Suitable\\': 4,\\n            \\'Not Suitable\\': 5\\n        }\\n        df[\\'suitability_rank\\'] = df[\\'suitability\\'].map(custom_order)\\n        sorted_df = df.sort_values(by=\\'suitability_rank\\')\\n        sorted_df = sorted_df.drop(columns=\\'suitability_rank\\')\\n        return sorted_df\\n\\n    sorted_df = sort_df_by_suitability()\\n\\n    filename = \"/final_user_df\"\\n    \\n    sorted_df.to_parquet(SAVE_PATH + f\"{filename}.parquet\", index=False)\\n\\n\\n    logging.info(f\"SORTED DF:\\n {sorted_df}. \\n This df has been saved in ...{filename}.parquet\")\\n\\nif __name__ == \"__main__\":\\n\\tasyncio.run(main())\\n        '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "async def main():\n",
    "\n",
    "    #TODO: This needs to be user's cv\n",
    "    #checked_json = await check_output_GPT4(input_cv=abstract_cv, min_n=0, top_n=10)\n",
    "\n",
    "    #Try block needs to be here\n",
    "\n",
    "    def ids_json_loads(data: list[dict[str, str, str]] = None) -> str:\n",
    "        if data is None:\n",
    "            data = checked_json\n",
    "            logging.info(f\"type of the json object: {type(data)} Data: {data}\")\n",
    "            #print(type(exp), exp)\n",
    "        \n",
    "        ids = \"\"\n",
    "        for item in data:\n",
    "            if \"id\" in item:\n",
    "                if ids:\n",
    "                    ids += \", \"\n",
    "                ids += f\"'{item['id']}'\"\n",
    "\n",
    "        return f\"({ids})\"\n",
    "\n",
    "    ids_ready = ids_json_loads()\n",
    "    logging.info(f\"Getting the ids from the json object: {type(ids_ready)}, {ids_ready}\")\n",
    "\n",
    "    def ids_json_loads(df: pd.DataFrame = df_most_suitable) -> str:\n",
    "\n",
    "        ids = \"\"\n",
    "        for _, row in df.iterrows():\n",
    "            if \"id\" in row:\n",
    "                if ids:\n",
    "                    ids += \", \"\n",
    "                ids += f\"'{row['id']}'\"\n",
    "\n",
    "        return f\"({ids})\"\n",
    "\n",
    "    ids_ready = ids_json_loads()\n",
    "    logging.info(f\"Getting the ids from the json object: {type(ids_ready)}, {ids_ready}\")\n",
    "\n",
    "\n",
    "    def find_jobs_per_ids(ids:str, table: str = \"test\") -> pd.DataFrame:\n",
    "        conn = psycopg2.connect(user=user, password=password, host=host, port=port, database=database)\n",
    "        # Create a cursor object\n",
    "        cur = conn.cursor()\n",
    "        #TABLE SHOULD EITHER BE \"main_jobs\" or \"test\"\n",
    "        cur.execute( f\"SELECT id, title, link, location FROM {table} WHERE id IN {ids}\")\n",
    "\n",
    "        # Fetch all rows from the table\n",
    "        rows = cur.fetchall()\n",
    "\n",
    "        # Separate the columns into individual lists\n",
    "        all_ids = [row[0] for row in rows]\n",
    "        all_titles = [row[1] for row in rows]\n",
    "        all_links = [row[2] for row in rows]\n",
    "        all_locations = [row[3] for row in rows]\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            'id': all_ids,\n",
    "            'title': all_titles,\n",
    "            'link': all_links,\n",
    "            'location': all_locations\n",
    "        })\n",
    "\n",
    "\n",
    "        # Close the database connection\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "        return df\n",
    "\n",
    "    df_postgre = find_jobs_per_ids(ids=ids_ready)\n",
    "    #Read the parquet\n",
    "    df_summaries = pd.read_parquet(SAVE_PATH + \"/summaries.parquet\")\n",
    "\n",
    "    df = df_postgre.merge(df_summaries, on='id', how='inner')\n",
    "\n",
    "    logging.info(f\"RELATED JOBS & THEIR SUMMARIES: \\n {df}\")\n",
    "\n",
    "    def adding_all_data(df: pd.DataFrame, suitable_jobs: list) -> pd.DataFrame:\n",
    "        for index, row in df.iterrows():\n",
    "            entry_id = row['id']\n",
    "            for json_item in suitable_jobs:\n",
    "                if int(json_item['id']) == entry_id:\n",
    "                    suitability = json_item['suitability']\n",
    "                    explanation = json_item['explanation']\n",
    "                    df.at[index, 'suitability'] = suitability\n",
    "                    df.at[index, 'explanation'] = explanation\n",
    "                    break\n",
    "        return df\n",
    "\n",
    "    updated_data = adding_all_data(df=df, suitable_jobs=checked_json)\n",
    "\n",
    "    logging.info(f\"ALL COLUMNS: \\n {updated_data}\")\n",
    "\n",
    "    def sort_df_by_suitability(df: pd.DataFrame = df) -> pd.DataFrame:\n",
    "        custom_order = {\n",
    "            'Highly Suitable': 1,\n",
    "            'Moderately Suitable': 2,\n",
    "            'Potentially Suitable': 3,\n",
    "            'Marginally Suitable': 4,\n",
    "            'Not Suitable': 5\n",
    "        }\n",
    "        df['suitability_rank'] = df['suitability'].map(custom_order)\n",
    "        sorted_df = df.sort_values(by='suitability_rank')\n",
    "        sorted_df = sorted_df.drop(columns='suitability_rank')\n",
    "        return sorted_df\n",
    "\n",
    "    sorted_df = sort_df_by_suitability()\n",
    "\n",
    "    filename = \"/final_user_df\"\n",
    "    \n",
    "    sorted_df.to_parquet(SAVE_PATH + f\"{filename}.parquet\", index=False)\n",
    "\n",
    "\n",
    "    logging.info(f\"SORTED DF:\\n {sorted_df}. \\n This df has been saved in ...{filename}.parquet\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\tasyncio.run(main())\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
