{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial\n",
    "import pretty_errors\n",
    "import timeit\n",
    "from preprocess import individual_preprocess\n",
    "from dotenv import load_dotenv\n",
    "from utils.SummariseJob import summarise_job_gpt\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "SAVE_PATH = os.getenv(\"SAVE_PATH\")\n",
    "E5_BASE_TOTAL_JOBS = os.getenv(\"E5_BASE_TOTAL_JOBS\")\n",
    "OPENAI_TOTAL_JOBS = os.getenv(\"OPENAI_TOTAL_JOBS\")\n",
    "\n",
    "#Start the timer\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "#GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "GPT_MODEL = \"gpt-4\"\n",
    "#GPT_MODEL = \"gpt-3.5-turbo-16k\"\n",
    "\"\"\"\"\n",
    "Load the embedded file\n",
    "\"\"\"\n",
    "\n",
    "embeddings_path = E5_BASE_TOTAL_JOBS\n",
    "\n",
    "df = pd.read_parquet(embeddings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor,\n",
    "                attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_ranked_by_relatedness_e5(query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    #Modify this to get more jobs\n",
    "    top_n: int = 10\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\n",
    "    model = AutoModel.from_pretrained('intfloat/e5-base-v2')\n",
    "\n",
    "    query_preprocess = individual_preprocess(query)\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    batch_dict = tokenizer(query_preprocess, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**batch_dict)\n",
    "    query_embedding = average_pool(outputs.last_hidden_state, batch_dict['attention_mask']).detach().numpy().flatten()\n",
    "    ids_and_relatednesses = [\n",
    "        (row[\"ids\"], relatedness_fn(query_embedding, row[\"embeddings\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    ids_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    ids, relatednesses = zip(*ids_and_relatednesses)\n",
    "    return ids[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiktoken function -> to count tokens\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delimiters = \"####\"\n",
    "\n",
    "system_prompt=f\"\"\"\n",
    "\n",
    "You are a job recruiter for a large recruitment agency./\n",
    "You will be provided with a candidate's CV./\n",
    "The CV will be delimited with {delimiters} characters./\n",
    "You will also be provided with the Job IDs (delimited by angle brackets) /\n",
    "and corresponding descriptions (delimited by triple dashes)/\n",
    "for the available job openings./\n",
    "\n",
    "Perform the following steps:/\n",
    "\n",
    "Step 1 - Classify the provided CV into a suitability category for each job opening./\n",
    "Step 2 - For each ID briefly explain in one sentence your reasoning behind the chosen suitability category./\n",
    "Step 3 - Only provide your output in json format with the keys: id, suitability and explanation./\n",
    "\n",
    "Do not classify a CV into a suitability category until you have classify the CV yourself.\n",
    "\n",
    "Suitability categories: Highly Suitable, Moderately Suitable,/\n",
    "Potentially Suitable, Marginally Suitable and Not Suitable./\n",
    "\n",
    "Highly Suitable: CVs in this category closely align with the job opening, demonstrating extensive relevant experience, skills, and qualifications. The candidate possesses all or most of the necessary requirements and is an excellent fit for the role./\n",
    "Moderately Suitable: CVs falling into this category show a reasonable match to the job opening. The candidate possesses some relevant experience, skills, and qualifications that align with the role, but there may be minor gaps or areas for improvement. With some additional training or development, they could become an effective candidate./\n",
    "Potentially Suitable: CVs in this category exhibit potential and may possess transferable skills or experience that could be valuable for the job opening. Although they may not meet all the specific requirements, their overall profile suggests that they could excel with the right support and training./\n",
    "Marginally Suitable: CVs falling into this category show limited alignment with the job opening. The candidate possesses a few relevant skills or experience, but there are significant gaps or deficiencies in their qualifications. They may require substantial training or experience to meet the requirements of the role./\n",
    "Not Suitable: CVs in this category do not match the requirements and qualifications of the job opening. The candidate lacks the necessary skills, experience, or qualifications, making them unsuitable for the role./\n",
    "\"\"\"\n",
    "\n",
    "introduction_prompt = \"\"\"\n",
    "\n",
    "\n",
    "\\n Available job openings:\\n\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "abstract_cv_past = \"\"\"Data Analyst: Cleansed, analyzed, and visualized data using Python, SQL Server, and Power BI.\n",
    "Legal Assistant: Drafted legal documents, collaborated on negotiation outlines, and handled trademark registrations.\n",
    "Data Analyst Jr.: Implemented A/B testing, utilized data analysis tools, and developed real-time visualizations.\n",
    "Special Needs Counselor: Led and assisted individuals with disabilities, provided personal care, and facilitated camp activities.\n",
    "Total years of professional experience: 3 years.\"\"\"\n",
    "\n",
    "abstract_cv = \"\"\"('Qualifications: \\n- LLB Law degree from Universidad de las Américas Puebla (UDLAP) with an accumulated average of 9.4/10.\\n- Currently on an international exchange at the University of Bristol for the final year of studying Law.\\n- Member of the Honours Program at UDLAP, conducting research on FinTech, Financial Inclusion, Blockchain, Cryptocurrencies, and Smart Contracts.\\n\\nPrevious job titles:\\n- Data Analyst at Tata Consultancy Services México, where I cleansed, interpreted, and analyzed data using Python and SQL Server to produce visual reports with Power BI.\\n- Legal Assistant at BLACKSHIIP Venture Capital, responsible for proofreading and drafting legal documents, as well as assisting with negotiations of International Share Purchase Agreements.\\n\\nResponsibilities/Key Duties:\\n- Developed and introduced A/B testing to make data-driven business decisions as a Data Analyst Jr. at AMATL GRÁFICOS.\\n- Taught mental arithmetic as a Mathematics Instructor at ALOHA Mental Arithmetic.\\n- Led and assisted individuals with physical and mental disabilities as a Special Needs Counsellor at Camp Merrywood and YMCA Camp Independence.\\n\\nSkills:\\n- Proficient in Python, SQL Server, Tableau, Power BI, Bash/Command Line, Git & GitHub, and Office 365.\\n- Strong written and verbal communication skills, teamwork, ability to work under pressure, attention to detail, and leadership skills.\\n- Knowledge in machine learning, probabilities & statistics, and proofreading.\\n\\nOther Achievements:\\n- Published paper on \"Smart Legal Contracts: From Theory to Reality\" and participated in the IDEAS Summer Program on Intelligence, Data, Ethics, and Society at the University of California, San Diego.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    #Return a message for GPT, with relevant source texts pulled from a dataframe\n",
    "    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = introduction_prompt\n",
    "    query_user = f\"{query}\"\n",
    "    message = introduction\n",
    "    total_cost_summarise_job = 0\n",
    "    for id in ids:\n",
    "        #Get the text for GPT to answer the question\n",
    "        job_description = df[df['ids'] == id]['text_data'].values[0] \n",
    "        \n",
    "        #Summarise the job description with GPT-3.5\n",
    "        job_summarised, cost = summarise_job_gpt(job_description)\n",
    "        total_cost_summarise_job += cost\n",
    "        # Add job descriptions to the message along with job ID\n",
    "        next_id = f'\\nID:<{id}>\\nJob Description:---{job_summarised}---\\n'\n",
    "        if (\n",
    "            num_tokens(message + next_id + query_user, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return query_user, message, total_cost_summarise_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def query_message(\\n    query: str,\\n    df: pd.DataFrame,\\n    model: str,\\n    token_budget: int\\n) -> str:\\n    #Return a message for GPT, with relevant source texts pulled from a dataframe.\\n    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df)\\n    #Basically giving the most relevant IDs from the previous function\\n    introduction = introduction_prompt\\n    query_user = f\"{query}\"\\n    message = introduction\\n    #total_cost_summarise_job = 0\\n    for id in ids:\\n        #Get the text for GPT to answer the question\\n        job_description = df[df[\\'ids\\'] == id][\\'text_data\\'].values[0] \\n        \\n        #Summarise the job description with GPT-3.5\\n        #job_summarised, cost = summarise_job(job_description)\\n        #total_cost_summarise_job += cost\\n        # Add job descriptions to the message along with job ID\\n        next_id = f\\'\\nID:<{id}>\\nJob Description:---{job_description}---\\n\\'\\n        if (\\n            num_tokens(message + next_id + query_user, model=model)\\n            > token_budget\\n        ):\\n            break\\n        else:\\n            message += next_id\\n    return query_user, message'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    #Return a message for GPT, with relevant source texts pulled from a dataframe.\n",
    "    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = introduction_prompt\n",
    "    query_user = f\"{query}\"\n",
    "    message = introduction\n",
    "    #total_cost_summarise_job = 0\n",
    "    for id in ids:\n",
    "        #Get the text for GPT to answer the question\n",
    "        job_description = df[df['ids'] == id]['text_data'].values[0] \n",
    "        \n",
    "        #Summarise the job description with GPT-3.5\n",
    "        #job_summarised, cost = summarise_job(job_description)\n",
    "        #total_cost_summarise_job += cost\n",
    "        # Add job descriptions to the message along with job ID\n",
    "        next_id = f'\\nID:<{id}>\\nJob Description:---{job_description}---\\n'\n",
    "        if (\n",
    "            num_tokens(message + next_id + query_user, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return query_user, message\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096,\n",
    "    print_gpt_messages: bool = True,\n",
    "    print_cost_and_relatednesses: bool = True\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    query_user, job_id_description, total_cost_summarise_job = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\n",
    "        {\"role\": \"assistant\", \"content\": job_id_description}\n",
    "    ]\n",
    "    if print_gpt_messages:\n",
    "        print(messages)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    if print_cost_and_relatednesses:\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        prompt_tokens = response['usage']['prompt_tokens']\n",
    "        completion_tokens = response['usage']['completion_tokens']\n",
    "        print(f\"\\nPROMPT TOKENS USED:{prompt_tokens}\\n\", f\"COMPLETION TOKENS USED:{completion_tokens}\\n\", f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\n",
    "        #Approximate cost\n",
    "        if GPT_MODEL == \"gpt-4\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.06, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            final_cost = total_cost_summarise_job + cost_classify\n",
    "            print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.002, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            final_cost = total_cost_summarise_job + cost_classify\n",
    "            print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.004, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            final_cost = total_cost_summarise_job + cost_classify\n",
    "            print(f\"FINAL COST: ${final_cost} USD\")\n",
    "\n",
    "        #relatednesses\n",
    "        ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df)\n",
    "        print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "        for id, relatedness in zip(ids, relatednesses):\n",
    "            print(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "\n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        print(\"\\n\", f\"DreamedJobAI finished! all in: {elapsed_time:.2f} seconds\", \"\\n\") \n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def ask(\\n    #This query is your question, only parameter to fill in function\\n    query: str,\\n    df: pd.DataFrame = df,\\n    model: str = GPT_MODEL,\\n    token_budget: int = 8192,\\n    print_gpt_messages: bool = True,\\n    print_cost_and_relatednesses: bool = True\\n) -> str:\\n    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\\n    query_user, job_id_description = query_message(query, df, model=model, token_budget=token_budget)\\n    messages = [\\n        {\"role\": \"system\", \"content\": system_prompt},\\n        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\\n        {\"role\": \"assistant\", \"content\": job_id_description}\\n    ]\\n    if print_gpt_messages:\\n        print(messages)\\n    response = openai.ChatCompletion.create(\\n        model=model,\\n        messages=messages,\\n        temperature=0\\n    )\\n    response_message = response[\"choices\"][0][\"message\"][\"content\"]\\n    \\n    if print_cost_and_relatednesses:\\n        total_tokens = response[\\'usage\\'][\\'total_tokens\\']\\n        prompt_tokens = response[\\'usage\\'][\\'prompt_tokens\\']\\n        completion_tokens = response[\\'usage\\'][\\'completion_tokens\\']\\n        print(f\"\\nPROMPT TOKENS USED:{prompt_tokens}\\n\", f\"COMPLETION TOKENS USED:{completion_tokens}\\n\", f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\\n        #Approximate cost\\n        if GPT_MODEL == \"gpt-4\":\\n            prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\\n            completion_cost = round((completion_tokens / 1000) * 0.06, 3)\\n            cost_classify = prompt_cost + completion_cost\\n            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\\n            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\\n            #final_cost = total_cost_summarise_job + cost_classify\\n            #print(f\"FINAL COST: ${final_cost} USD\")\\n        elif GPT_MODEL == \"gpt-3.5-turbo\":\\n            prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\\n            completion_cost = round((completion_tokens / 1000) * 0.002, 3)\\n            cost_classify = prompt_cost + completion_cost\\n            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\\n            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\\n            #final_cost = total_cost_summarise_job + cost_classify\\n            #print(f\"FINAL COST: ${final_cost} USD\")\\n        elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\\n            prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\\n            completion_cost = round((completion_tokens / 1000) * 0.004, 3)\\n            cost_classify = prompt_cost + completion_cost\\n            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\\n            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\\n            #final_cost = total_cost_summarise_job + cost_classify\\n            #print(f\"FINAL COST: ${final_cost} USD\")\\n\\n        #relatednesses\\n        ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df)\\n        print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\\n        for id, relatedness in zip(ids, relatednesses):\\n            print(f\"ID: {id} has the following {relatedness=:.3f}\")\\n        \\n        elapsed_time = timeit.default_timer() - start_time\\n        print(\"\\n\", f\"DreamedJobAI finished! all in: {elapsed_time:.2f} seconds\", \"\\n\") \\n    return response_message'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 8192,\n",
    "    print_gpt_messages: bool = True,\n",
    "    print_cost_and_relatednesses: bool = True\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    query_user, job_id_description = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\n",
    "        {\"role\": \"assistant\", \"content\": job_id_description}\n",
    "    ]\n",
    "    if print_gpt_messages:\n",
    "        print(messages)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    if print_cost_and_relatednesses:\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        prompt_tokens = response['usage']['prompt_tokens']\n",
    "        completion_tokens = response['usage']['completion_tokens']\n",
    "        print(f\"\\nPROMPT TOKENS USED:{prompt_tokens}\\n\", f\"COMPLETION TOKENS USED:{completion_tokens}\\n\", f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\n",
    "        #Approximate cost\n",
    "        if GPT_MODEL == \"gpt-4\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.06, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            #final_cost = total_cost_summarise_job + cost_classify\n",
    "            #print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.002, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            #final_cost = total_cost_summarise_job + cost_classify\n",
    "            #print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.004, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            #final_cost = total_cost_summarise_job + cost_classify\n",
    "            #print(f\"FINAL COST: ${final_cost} USD\")\n",
    "\n",
    "        #relatednesses\n",
    "        ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df)\n",
    "        print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "        for id, relatedness in zip(ids, relatednesses):\n",
    "            print(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "        \n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        print(\"\\n\", f\"DreamedJobAI finished! all in: {elapsed_time:.2f} seconds\", \"\\n\") \n",
    "    return response_message\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'role': 'system', 'content': \"\\n\\nYou are a job recruiter for a large recruitment agency./\\nYou will be provided with a candidate's CV./\\nThe CV will be delimited with #### characters./\\nYou will also be provided with the Job IDs (delimited by angle brackets) /\\nand corresponding descriptions (delimited by triple dashes)/\\nfor the available job openings./\\n\\nPerform the following steps:/\\n\\nStep 1 - Classify the provided CV into a suitability category for each job opening./\\nStep 2 - For each ID briefly explain in one sentence your reasoning behind the chosen suitability category./\\nStep 3 - Only provide your output in json format with the keys: id, suitability and explanation./\\n\\nDo not classify a CV into a suitability category until you have classify the CV yourself.\\n\\nSuitability categories: Highly Suitable, Moderately Suitable,/\\nPotentially Suitable, Marginally Suitable and Not Suitable./\\n\\nHighly Suitable: CVs in this category closely align with the job opening, demonstrating extensive relevant experience, skills, and qualifications. The candidate possesses all or most of the necessary requirements and is an excellent fit for the role./\\nModerately Suitable: CVs falling into this category show a reasonable match to the job opening. The candidate possesses some relevant experience, skills, and qualifications that align with the role, but there may be minor gaps or areas for improvement. With some additional training or development, they could become an effective candidate./\\nPotentially Suitable: CVs in this category exhibit potential and may possess transferable skills or experience that could be valuable for the job opening. Although they may not meet all the specific requirements, their overall profile suggests that they could excel with the right support and training./\\nMarginally Suitable: CVs falling into this category show limited alignment with the job opening. The candidate possesses a few relevant skills or experience, but there are significant gaps or deficiencies in their qualifications. They may require substantial training or experience to meet the requirements of the role./\\nNot Suitable: CVs in this category do not match the requirements and qualifications of the job opening. The candidate lacks the necessary skills, experience, or qualifications, making them unsuitable for the role./\\n\"}, {'role': 'user', 'content': '####(\\'Qualifications: \\n- LLB Law degree from Universidad de las Américas Puebla (UDLAP) with an accumulated average of 9.4/10.\\n- Currently on an international exchange at the University of Bristol for the final year of studying Law.\\n- Member of the Honours Program at UDLAP, conducting research on FinTech, Financial Inclusion, Blockchain, Cryptocurrencies, and Smart Contracts.\\n\\nPrevious job titles:\\n- Data Analyst at Tata Consultancy Services México, where I cleansed, interpreted, and analyzed data using Python and SQL Server to produce visual reports with Power BI.\\n- Legal Assistant at BLACKSHIIP Venture Capital, responsible for proofreading and drafting legal documents, as well as assisting with negotiations of International Share Purchase Agreements.\\n\\nResponsibilities/Key Duties:\\n- Developed and introduced A/B testing to make data-driven business decisions as a Data Analyst Jr. at AMATL GRÁFICOS.\\n- Taught mental arithmetic as a Mathematics Instructor at ALOHA Mental Arithmetic.\\n- Led and assisted individuals with physical and mental disabilities as a Special Needs Counsellor at Camp Merrywood and YMCA Camp Independence.\\n\\nSkills:\\n- Proficient in Python, SQL Server, Tableau, Power BI, Bash/Command Line, Git & GitHub, and Office 365.\\n- Strong written and verbal communication skills, teamwork, ability to work under pressure, attention to detail, and leadership skills.\\n- Knowledge in machine learning, probabilities & statistics, and proofreading.\\n\\nOther Achievements:\\n- Published paper on \"Smart Legal Contracts: From Theory to Reality\" and participated in the IDEAS Summer Program on Intelligence, Data, Ethics, and Society at the University of California, San Diego.####'}, {'role': 'assistant', 'content': \"\\n\\n\\n\\n Available job openings:\\n\\n\\n\\nID:<35204>\\nJob Description:---Job Information:\\n- Job Title: iOS Lead\\n- Job Objective: To secure the operation of mobile applications\\n- Responsibilities/Key Duties: \\n  - Coordinate with CTO and Software Architecture for technology and development processes\\n  - Ensure good development practices are followed\\n  - Coordinate staff assignments and conduct technical interviews\\n  - Conduct team performance evaluations and provide support\\n  - Make technical decisions and deal with customers\\n- Qualifications/Requirements: \\n  - More than five years of experience\\n  - Advanced English\\n  - Experience in leadership roles\\n  - Deep knowledge of Architectural patterns\\n  - Experience with UIKit, SwiftUI, CoreData, Combine, Mapkit, and Apple Watch development\\n  - Experience managing apps in the App Store\\n  - Hands-on experience in Unit Testing\\n- Preferred Skills/Experience: \\n  - Strong organizational skills\\n  - Curious, resourceful, and eager to tackle new challenges\\n  - Experience planning, designing, and implementing strategies for large-scale system software\\n  - Experience working with cross-functional teams in a fast-growing environment\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Umvel, part of NTT DATA\\n- Location: Mexico and the US\\n- Culture and Values: Committed to creating a more equitable world, no discrimination based on religion, age, gender, culture, or social status\\n- Compensation and Benefits: \\n  - Vacation after 6 months, vacation day by law, Christmas week off\\n  - Variable annual salary increase based on performance evaluation\\n  - English classes\\n  - Major medical expenses insurance\\n  - Medical discount card\\n  - Project participation bonus\\n  - Seniority bonus\\n  - Career growth path---\\n\\nID:<33815>\\nJob Description:---Job Information:\\n- Job Title: Analytics Engineering Lead\\n- Job Objective: To contribute to the design and scalability of data models that measure the effectiveness of fraud and financial crime controls, develop robust data models downstream of backend services, and help shape and maintain best practices for the Data Warehouse.\\n- Responsibilities/Key Duties: Serve as a data architect, develop data models in BigQuery, collaborate with the data squad, prioritize data governance issues, and continuously improve data practices.\\n- Qualifications/Requirements: Strong passion for data modeling, experience with ETL projects and Big Data, familiarity with SQL and data modeling, commitment to continuous improvement, and experience building robust and reliable data sets.\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Monzo\\n- Location: United Kingdom, with the option for distributed working within the UK and ad hoc meetings in London.\\n- Culture/Values: Monzo aims to make banking less complex and opaque, focusing on solving problems rather than selling financial products. They have an amazing community that suggests features and provides constant feedback.\\n- Compensation and Benefits: The job offers flexible working hours, the opportunity to relocate to the UK, and visa sponsorship.---\\n\\nID:<43812>\\nJob Description:---Job Information:\\n- Job Title: Financial Reporting Analyst\\n- Job Objective: The successful candidate will be responsible for preparing consolidated financial reporting, calculating and coordinating incentive settlements, developing an annual budget, monitoring progress, providing routine reporting, performing detailed analysis, creating standardized processes, and assisting in financial due diligence.\\n- Responsibilities/Key Duties: Prepare monthly consolidated financial reporting, analyze results, calculate incentive fees, establish reporting protocols, develop and monitor annual budgets, distribute monthly reports, maintain expenditure controls, support forecasting efforts, perform detailed analysis on performance, create executive summaries, assist in due diligence efforts, develop protocols and processes for data collection and financial reporting.\\n- Qualifications/Requirements: At least 2 years' experience in financial reporting consolidation, proficiency in Excel and PowerPoint, financial business acumen, knowledge of US partnerships, experience with process development and improvement, ability to prioritize tasks and work under time constraints, strong problem-solving and analytical skills, effective communication skills, ability to develop and maintain relationships with colleagues.\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Dentons, the world's largest law firm\\n- Location: Global presence\\n- Culture and Values: Dentons is known for delivering quality and value to clients, challenging the status quo, and advancing client interests in the communities they operate in.\\n- Compensation and Benefits: Not mentioned in the provided passage.---\\n\\nID:<35454>\\nJob Description:---Job Information:\\n- Job Title: Staff Software Engineer\\n- Job Objective: To lead a small team of backend and ML engineers in building highly scalable systems with micro-service architecture for new markets\\n- Responsibilities/Key Duties: Planning, designing, and building elegant solutions, managing the software development process, designing and implementing the overall architecture, ensuring speed and scalability, building REST APIs, implementing continuous integration and deployment, generating ideas for new initiatives and technologies, being a domain expert in the payment processing industry, communicating with stakeholders in various departments\\n- Qualifications/Requirements: Fluent in English, Bachelor's Degree or higher in a quantitative field, 8+ years of experience in backend software development, 6+ years of professional experience with Python, expertise in AWS tech stack and Terraform or related technologies\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Galileo\\n- Location: Not specified\\n- Culture/Values/Mission: Welcoming, collaborative, and focused on making an impact\\n- Financial Aspects: Not specified\\n- Additional Perks: Not specified---\\n\\nID:<44479>\\nJob Description:---Job Information:\\n- Job Title: Contract Customer Data Analyst\\n- Job Objective: The objective of this position is to collaborate with internal stakeholders and interface with Subscribe customers to understand their needs, analyze their existing product catalog and historical order details, and import that data into Subscribe.\\n- Responsibilities/Key Duties: The responsibilities include working directly with customers to understand their product catalogs and order history, transforming data into a format that can be imported into Subskribe, filling in gaps and inaccuracies in customer data, and collaborating with the engineering team to define requirements.\\n- Qualifications/Requirements: The qualifications for this position include 2-5 years of experience with data entry or analysis, preferably with financial data, proficiency in Microsoft Excel, Google Sheets, or similar spreadsheets, excellent communication skills, and the ability to work with data import tools.\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Subskribe Inc.\\n- Location: Worldwide Remote\\n- Company Culture/Values/Mission: Subskribe is a fast-moving, fast-growing company that is building the smartest, most scalable, and easiest-to-customize CPQ Subscription Billing and Revenue Recognition platform for SaaS companies. The company is committed to attracting, retaining, and developing a highly qualified, diverse, and dedicated team.\\n- Compensation and Benefits: The job is a 3-6 month contract with the potential to convert to full-time. The company is well-funded by top Silicon Valley investors. Additional perks and financial aspects are not mentioned in the provided job opening.---\\n\\nID:<44194>\\nJob Description:---Job Information:\\n- Job Title: Business Analyst\\n- Job Objective: The Business Analyst will be responsible for evaluating business processes, identifying areas for improvement, and implementing solutions. They will also stay up-to-date on the latest process and IT advancements, conduct meetings and presentations, perform requirements analysis, and communicate insights and plans to cross-functional team members and management. Additionally, they will gather critical information, produce useful reports, provide leadership training, allocate resources, and ensure solutions meet business needs and requirements. The Business Analyst will also manage projects, update and maintain procedures, prioritize initiatives, and serve as a liaison between stakeholders and users.\\n- Responsibilities/Key Duties: Evaluating business processes, developing and implementing solutions, conducting meetings and presentations, performing requirements analysis, documenting and communicating results, gathering critical information, providing leadership training, allocating resources, ensuring solutions meet business needs, performing user acceptance testing, managing projects, updating and maintaining procedures, prioritizing initiatives, serving as a liaison, managing competing resources and priorities, monitoring deliverables, and ensuring timely completion of projects.\\n- Qualifications/Requirements: A bachelor's degree in business or related field or an MBA, a minimum of 5 years of experience in business analysis or a related field, exceptional analytical and conceptual thinking skills, the ability to influence stakeholders, advanced technical skills, excellent documentation skills, experience creating detailed reports and giving presentations, competency in Microsoft applications, and excellent planning, organizational, and time management skills.\\n- Preferred Skills/Experience: Not mentioned in the job opening.\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Nisum\\n- Location: California, United States\\n- Culture/Values/Mission: Nisum is a leading global digital commerce firm that focuses on building success together with its customers. They enable clients to achieve direct business growth by building advanced technology solutions that provide immersive and seamless experiences across digital and physical channels.\\n- Compensation and Benefits: No information provided in the job opening.---\\n\\nID:<35904>\\nJob Description:---Job Information:\\n- Job Title: Junior AML Analyst\\n- Job Objective: To support the AML Team in ensuring effective client and matter risk controls are in place and operate across the full range of practice areas within the firm globally. To contribute to ensuring that the firm complies with its regulatory obligations and appropriately manages and mitigates risk issues.\\n- Responsibilities/Key Duties: Conduct client due diligence (CDD) on new and existing clients and matters, undertake client and matter risk assessments, assist with ongoing monitoring of existing clients and matters, undertake sanction update screening, advise on matter opening processes, assist with internal audit processes, review and update internal compliance policies and procedures, assist with projects and ad hoc assignments.\\n\\nQualifications/Requirements:\\n- Relevant university degree\\n- First legal or compliance experience working in a law firm or professional services environment would be an asset\\n- Highly effective research skills\\n- Good knowledge of relevant compliance rules and regulations\\n- Strong problem-solving and analytical skills\\n- Excellent written and verbal communication skills\\n- Customer-facing skills\\n- Efficient and organized approach to administration\\n\\nPreferred Skills/Experience: Dynamic and flexible team players with highly developed critical thinking, ability to assess legal and commercial risks and advise on appropriate mitigation strategies.\\n\\nAbout the Company and Compensation and Benefits:\\n- Company Name: Dentons\\n- Location: Czechia\\n- Dentons is the world's largest law firm delivering quality and value to clients.\\n- Additional perks and benefits not mentioned.---\\n\\nID:<44444>\\nJob Description:---Job Information:\\n- Job Title: Analyst Customer 360 Insights\\n- Job Objective: Improve and scale Upwork customer experience through actionable insights\\n- Responsibilities/Key Duties: \\n  - Conduct data analysis and develop dashboards\\n  - Develop and manage ETL processes\\n  - Perform advanced analytics to generate insights\\n  - Refine questions and generate hypotheses about the product and business\\n  - Create customer experience journey maps\\n  - Identify opportunities for automation and self-serve mechanisms\\n- Qualifications/Requirements: \\n  - Highly analytical and creative in turning large data into insights\\n  - Ability to work well with cross-functional stakeholders\\n  - Experience in data analysis and communicating insights\\n  - Expertise in SQL, ETL, statistical analysis, and data visualization tools\\n  - Strong grasp of statistical concepts and advanced analytics\\n  - Experience with self-serve reports and visualizations\\n  - Passion for using data and analytics to support business decisions\\n  - Prior experience in Analytics, Business Intelligence, or Data Science preferred\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Upwork\\n- Location: South America\\n- Culture and Values: Upwork is committed to fostering a diverse and inclusive workforce\\n- Compensation and Benefits: Not mentioned in the job opening.---\\n\\nID:<44446>\\nJob Description:---Job Information:\\n- Job Title: Staff Software Engineer\\n- Job Objective: The Staff Software Engineer will be responsible for architecting stream processing features, developing robust data pipelines, designing and building experimentation infrastructure, and improving the reliability and efficiency of core data processing systems. They will also work cross-functionally with various teams to identify and execute new opportunities in experimentation.\\n- Responsibilities/Key Duties: Architect stream processing features, develop data pipelines, design and build experimentation infrastructure, improve reliability and efficiency of data processing systems, work cross-functionally with other teams, provide statistical analysis and visualization tooling.\\n- Qualifications/Requirements: 8 years of industry experience in building large-scale production systems, experience with stream processing systems, solid understanding of databases, experience with data warehouse solutions, experience with statistical analysis tooling, experience leading technical projects, BS/MS/PhD in Computer Science or related field.\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Affirm\\n- Location: Spain\\n- Culture/Values/Mission: Affirm is reinventing credit to make it more honest and friendly, providing consumers with the flexibility to buy now and pay later without hidden fees or compounding interest.\\n- Compensation and Benefits: Affirm offers a remotefirst work environment, with the majority of roles being remote. Some roles may require occasional work from an assigned office. The company has a transparent compensation structure based on job-related skills, experience, and education. Certain roles are eligible for equity awards based on performance and tenure milestones. Visa sponsorship is available for the role, but the candidate must be based in Spain.---\\n\\nID:<36697>\\nJob Description:---Job Information:\\n- Job Title: DBA Oracle Remoto MX\\n- Job Objective: The company is seeking a DBA Oracle with certifications in ITIL Foundation V3 or higher, Oracle Database Administrator V11g or higher, Oracle 12C Administrator or higher, and Oracle GoldenGate 10 or higher. The position requires expertise in staffing projects, headhunting, and recruitment of IT specialists.\\n- Responsibilities/Key Duties: The DBA Oracle will be responsible for managing Oracle databases, ensuring their performance, security, and availability. They will also be involved in project staffing, headhunting, and recruitment of IT specialists.\\n- Qualifications/Requirements: The candidate must have a valid certification in ITIL Foundation V3 or higher, Oracle Database Administrator V11g or higher, Oracle 12C Administrator or higher, and Oracle GoldenGate 10 or higher.\\n\\nPreferred Skills/Experience: Not mentioned in the provided passage.\\n\\nAbout the Company and Compensation and Benefits:\\n- Company: ITC Home\\n- Location: Remote/Anywhere in Mexico\\n- Culture/Values/Mission: Not mentioned in the provided passage.\\n- Compensation and Benefits: The company offers a competitive salary of $40,000.00 - $52,000.00 per month, along with statutory and superior benefits such as a savings account, discounts, preferential prices, major medical expenses insurance, life insurance, and the opportunity to work from home.---\\n\"}]\n",
      "\n",
      "PROMPT TOKENS USED:3841\n",
      " COMPLETION TOKENS USED:471\n",
      " \n",
      "TOTAL TOKENS USED:4312\n",
      "\n",
      "COST FOR CLASSIFYING: $0.14300000000000002 USD\n",
      "COST FOR SUMMARISING: $0.033 USD\n",
      "FINAL COST: $0.17600000000000002 USD\n",
      "\n",
      "THE IDs ARE RANKED BY RELEVANCE:\n",
      "\n",
      "ID: 35204 has the following relatedness=0.870\n",
      "ID: 33815 has the following relatedness=0.868\n",
      "ID: 43812 has the following relatedness=0.865\n",
      "ID: 35454 has the following relatedness=0.862\n",
      "ID: 44479 has the following relatedness=0.860\n",
      "ID: 44194 has the following relatedness=0.860\n",
      "ID: 35904 has the following relatedness=0.859\n",
      "ID: 44444 has the following relatedness=0.859\n",
      "ID: 44446 has the following relatedness=0.859\n",
      "ID: 36697 has the following relatedness=0.858\n",
      "\n",
      " DreamedJobAI finished! all in: 87.26 seconds \n",
      "\n",
      "[{\"id\": \"35204\", \"suitability\": \"Not Suitable\", \"explanation\": \"The candidate does not have the required experience in iOS development, leadership roles, or the specific technical skills listed in the job description.\"},\n",
      "{\"id\": \"33815\", \"suitability\": \"Highly Suitable\", \"explanation\": \"The candidate has relevant experience in data analysis, proficiency in Python and SQL, and knowledge in machine learning, which aligns with the job's requirements.\"},\n",
      "{\"id\": \"43812\", \"suitability\": \"Potentially Suitable\", \"explanation\": \"The candidate has experience in data analysis and legal assistance, which could be transferable to the financial reporting role, but lacks specific experience in financial reporting consolidation.\"},\n",
      "{\"id\": \"35454\", \"suitability\": \"Moderately Suitable\", \"explanation\": \"The candidate has extensive experience in backend software development and proficiency in Python, but lacks specific experience in payment processing industry.\"},\n",
      "{\"id\": \"44479\", \"suitability\": \"Highly Suitable\", \"explanation\": \"The candidate's experience as a Data Analyst and proficiency in Python, SQL, and data visualization tools aligns well with the job's requirements.\"},\n",
      "{\"id\": \"44194\", \"suitability\": \"Potentially Suitable\", \"explanation\": \"The candidate has experience in data analysis and teaching, which could be transferable to the business analyst role, but lacks specific experience in business analysis.\"},\n",
      "{\"id\": \"35904\", \"suitability\": \"Highly Suitable\", \"explanation\": \"The candidate's law degree and experience in a legal assistant role aligns well with the job's requirements for legal or compliance experience.\"},\n",
      "{\"id\": \"44444\", \"suitability\": \"Highly Suitable\", \"explanation\": \"The candidate's experience in data analysis, proficiency in Python and SQL, and knowledge in machine learning aligns well with the job's requirements.\"},\n",
      "{\"id\": \"44446\", \"suitability\": \"Moderately Suitable\", \"explanation\": \"The candidate has extensive experience in backend software development and proficiency in Python, but lacks specific experience in building large-scale production systems.\"},\n",
      "{\"id\": \"36697\", \"suitability\": \"Not Suitable\", \"explanation\": \"The candidate does not have the required certifications or experience in Oracle Database Administration.\"}]\n"
     ]
    }
   ],
   "source": [
    "print(ask(abstract_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_3_summary = \"\"\"Write an abstract description of the following CV in four bullet points. \n",
    "\n",
    "Let's think step by step to summarise it into four bullet points:\n",
    "\n",
    "1. Focus on the main skills and responsibilities of each role. \n",
    "2. One of the bullet points is the total years of experience.\n",
    "2. Omit the employer names. \n",
    "3. Your answer must be in an active voice. \n",
    "4. Double-check that the summary is in four bullet points, three summarising the main skills and responsibilities and the last one is about the years of experience. \n",
    "CV IS BELOW: \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
