{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import psycopg2\n",
    "import pandas as pd\n",
    "import tiktoken  # for counting tokens\n",
    "from scipy import spatial\n",
    "import pretty_errors\n",
    "import timeit\n",
    "import time\n",
    "from openai.error import OpenAIError\n",
    "import json\n",
    "from typing import Callable\n",
    "from preprocess import individual_preprocess\n",
    "from dotenv import load_dotenv\n",
    "from utils.SummariseJob import summarise_job_gpt\n",
    "from torch import Tensor\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv('.env')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "user = os.getenv(\"user\")\n",
    "password = os.getenv(\"password\")\n",
    "host = os.getenv(\"host\")\n",
    "port = os.getenv(\"port\")\n",
    "database = os.getenv(\"database\")\n",
    "SAVE_PATH = os.getenv(\"SAVE_PATH\")\n",
    "E5_BASE_TOTAL_JOBS = os.getenv(\"E5_BASE_TOTAL_JOBS\")\n",
    "OPENAI_TOTAL_JOBS = os.getenv(\"OPENAI_TOTAL_JOBS\")\n",
    "\n",
    "#Start the timer\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "#GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "GPT_MODEL = \"gpt-4\"\n",
    "#GPT_MODEL = \"gpt-3.5-turbo-16k\"\n",
    "\"\"\"\"\n",
    "Load the embedded file\n",
    "\"\"\"\n",
    "\n",
    "embeddings_path = E5_BASE_TOTAL_JOBS\n",
    "\n",
    "df = pd.read_parquet(embeddings_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states: Tensor,\n",
    "                attention_mask: Tensor) -> Tensor:\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_ranked_by_relatedness_e5(query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    #Modify this to get more jobs\n",
    "    top_n: int = 10\n",
    ") -> tuple[list[str], list[float]]:\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')\n",
    "    model = AutoModel.from_pretrained('intfloat/e5-base-v2')\n",
    "\n",
    "    query_preprocess = individual_preprocess(query)\n",
    "    \n",
    "    # Tokenize the input texts\n",
    "    batch_dict = tokenizer(query_preprocess, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "    outputs = model(**batch_dict)\n",
    "    query_embedding = average_pool(outputs.last_hidden_state, batch_dict['attention_mask']).detach().numpy().flatten()\n",
    "    ids_and_relatednesses = [\n",
    "        (row[\"ids\"], relatedness_fn(query_embedding, row[\"embeddings\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    ids_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    ids, relatednesses = zip(*ids_and_relatednesses)\n",
    "    return ids[:top_n], relatednesses[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiktoken function -> to count tokens\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "delimiters = \"####\"\n",
    "\n",
    "system_prompt=f\"\"\"\n",
    "\n",
    "You are a job recruiter for a large recruitment agency./\n",
    "You will be provided with a candidate's CV./\n",
    "The CV will be delimited with {delimiters} characters./\n",
    "You will also be provided with the Job IDs (delimited by angle brackets) /\n",
    "and corresponding descriptions (delimited by triple dashes)/\n",
    "for the available job openings./\n",
    "\n",
    "Perform the following steps:/\n",
    "\n",
    "Step 1 - Classify the provided CV into a suitability category for each job opening./\n",
    "Step 2 - For each ID briefly explain in one sentence your reasoning behind the chosen suitability category./\n",
    "Step 3 - Only provide your output in json format with the keys: id, suitability and explanation./\n",
    "\n",
    "Do not classify a CV into a suitability category until you have classify the CV yourself.\n",
    "\n",
    "Suitability categories: Highly Suitable, Moderately Suitable,/\n",
    "Potentially Suitable, Marginally Suitable and Not Suitable./\n",
    "\n",
    "Highly Suitable: CVs in this category closely align with the job opening, demonstrating extensive relevant experience, skills, and qualifications. The candidate possesses all or most of the necessary requirements and is an excellent fit for the role./\n",
    "Moderately Suitable: CVs falling into this category show a reasonable match to the job opening. The candidate possesses some relevant experience, skills, and qualifications that align with the role, but there may be minor gaps or areas for improvement. With some additional training or development, they could become an effective candidate./\n",
    "Potentially Suitable: CVs in this category exhibit potential and may possess transferable skills or experience that could be valuable for the job opening. Although they may not meet all the specific requirements, their overall profile suggests that they could excel with the right support and training./\n",
    "Marginally Suitable: CVs falling into this category show limited alignment with the job opening. The candidate possesses a few relevant skills or experience, but there are significant gaps or deficiencies in their qualifications. They may require substantial training or experience to meet the requirements of the role./\n",
    "Not Suitable: CVs in this category do not match the requirements and qualifications of the job opening. The candidate lacks the necessary skills, experience, or qualifications, making them unsuitable for the role./\n",
    "\"\"\"\n",
    "\n",
    "introduction_prompt = \"\"\"\n",
    "\n",
    "\n",
    "\\n Available job openings:\\n\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "abstract_cv_past = \"\"\"Data Analyst: Cleansed, analyzed, and visualized data using Python, SQL Server, and Power BI.\n",
    "Legal Assistant: Drafted legal documents, collaborated on negotiation outlines, and handled trademark registrations.\n",
    "Data Analyst Jr.: Implemented A/B testing, utilized data analysis tools, and developed real-time visualizations.\n",
    "Special Needs Counselor: Led and assisted individuals with disabilities, provided personal care, and facilitated camp activities.\n",
    "Total years of professional experience: 3 years.\"\"\"\n",
    "\n",
    "abstract_cv = \"\"\"('Qualifications: \\n- LLB Law degree from Universidad de las Américas Puebla (UDLAP) with an accumulated average of 9.4/10.\\n- Currently on an international exchange at the University of Bristol for the final year of studying Law.\\n- Member of the Honours Program at UDLAP, conducting research on FinTech, Financial Inclusion, Blockchain, Cryptocurrencies, and Smart Contracts.\\n\\nPrevious job titles:\\n- Data Analyst at Tata Consultancy Services México, where I cleansed, interpreted, and analyzed data using Python and SQL Server to produce visual reports with Power BI.\\n- Legal Assistant at BLACKSHIIP Venture Capital, responsible for proofreading and drafting legal documents, as well as assisting with negotiations of International Share Purchase Agreements.\\n\\nResponsibilities/Key Duties:\\n- Developed and introduced A/B testing to make data-driven business decisions as a Data Analyst Jr. at AMATL GRÁFICOS.\\n- Taught mental arithmetic as a Mathematics Instructor at ALOHA Mental Arithmetic.\\n- Led and assisted individuals with physical and mental disabilities as a Special Needs Counsellor at Camp Merrywood and YMCA Camp Independence.\\n\\nSkills:\\n- Proficient in Python, SQL Server, Tableau, Power BI, Bash/Command Line, Git & GitHub, and Office 365.\\n- Strong written and verbal communication skills, teamwork, ability to work under pressure, attention to detail, and leadership skills.\\n- Knowledge in machine learning, probabilities & statistics, and proofreading.\\n\\nOther Achievements:\\n- Published paper on \"Smart Legal Contracts: From Theory to Reality\" and participated in the IDEAS Summer Program on Intelligence, Data, Ethics, and Society at the University of California, San Diego.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    #Return a message for GPT, with relevant source texts pulled from a dataframe\n",
    "    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = introduction_prompt\n",
    "    query_user = f\"{query}\"\n",
    "    message = introduction\n",
    "    total_cost_summarise_job = 0\n",
    "    for id in ids:\n",
    "        #Get the text for GPT to answer the question\n",
    "        job_description = df[df['ids'] == id]['text_data'].values[0] \n",
    "        \n",
    "        #Summarise the job description with GPT-3.5\n",
    "        job_summarised, cost = summarise_job_gpt(job_description)\n",
    "        total_cost_summarise_job += cost\n",
    "        # Add job descriptions to the message along with job ID\n",
    "        next_id = f'\\nID:<{id}>\\nJob Description:---{job_summarised}---\\n'\n",
    "        if (\n",
    "            num_tokens(message + next_id + query_user, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return query_user, message, total_cost_summarise_job\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def query_message(\\n    query: str,\\n    df: pd.DataFrame,\\n    model: str,\\n    token_budget: int\\n) -> str:\\n    #Return a message for GPT, with relevant source texts pulled from a dataframe.\\n    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df)\\n    #Basically giving the most relevant IDs from the previous function\\n    introduction = introduction_prompt\\n    query_user = f\"{query}\"\\n    message = introduction\\n    #total_cost_summarise_job = 0\\n    for id in ids:\\n        #Get the text for GPT to answer the question\\n        job_description = df[df[\\'ids\\'] == id][\\'text_data\\'].values[0] \\n        \\n        #Summarise the job description with GPT-3.5\\n        #job_summarised, cost = summarise_job(job_description)\\n        #total_cost_summarise_job += cost\\n        # Add job descriptions to the message along with job ID\\n        next_id = f\\'\\nID:<{id}>\\nJob Description:---{job_description}---\\n\\'\\n        if (\\n            num_tokens(message + next_id + query_user, model=model)\\n            > token_budget\\n        ):\\n            break\\n        else:\\n            message += next_id\\n    return query_user, message'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    #Return a message for GPT, with relevant source texts pulled from a dataframe.\n",
    "    ids, relatednesses = ids_ranked_by_relatedness_e5(query, df)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = introduction_prompt\n",
    "    query_user = f\"{query}\"\n",
    "    message = introduction\n",
    "    #total_cost_summarise_job = 0\n",
    "    for id in ids:\n",
    "        #Get the text for GPT to answer the question\n",
    "        job_description = df[df['ids'] == id]['text_data'].values[0] \n",
    "        \n",
    "        #Summarise the job description with GPT-3.5\n",
    "        #job_summarised, cost = summarise_job(job_description)\n",
    "        #total_cost_summarise_job += cost\n",
    "        # Add job descriptions to the message along with job ID\n",
    "        next_id = f'\\nID:<{id}>\\nJob Description:---{job_description}---\\n'\n",
    "        if (\n",
    "            num_tokens(message + next_id + query_user, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return query_user, message\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096,\n",
    "    print_gpt_messages: bool = True,\n",
    "    print_cost_and_relatednesses: bool = True\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    query_user, job_id_description, total_cost_summarise_job = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\n",
    "        {\"role\": \"assistant\", \"content\": job_id_description}\n",
    "    ]\n",
    "    if print_gpt_messages:\n",
    "        print(messages)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    if print_cost_and_relatednesses:\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        prompt_tokens = response['usage']['prompt_tokens']\n",
    "        completion_tokens = response['usage']['completion_tokens']\n",
    "        print(f\"\\nPROMPT TOKENS USED:{prompt_tokens}\\n\", f\"COMPLETION TOKENS USED:{completion_tokens}\\n\", f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\n",
    "        #Approximate cost\n",
    "        if GPT_MODEL == \"gpt-4\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.06, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            final_cost = total_cost_summarise_job + cost_classify\n",
    "            print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.002, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            final_cost = total_cost_summarise_job + cost_classify\n",
    "            print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.004, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            final_cost = total_cost_summarise_job + cost_classify\n",
    "            print(f\"FINAL COST: ${final_cost} USD\")\n",
    "\n",
    "        #relatednesses\n",
    "        ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df)\n",
    "        print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "        for id, relatedness in zip(ids, relatednesses):\n",
    "            print(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "\n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        print(\"\\n\", f\"DreamedJobAI finished! all in: {elapsed_time:.2f} seconds\", \"\\n\") \n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'def ask(\\n    #This query is your question, only parameter to fill in function\\n    query: str,\\n    df: pd.DataFrame = df,\\n    model: str = GPT_MODEL,\\n    token_budget: int = 8192,\\n    print_gpt_messages: bool = True,\\n    print_cost_and_relatednesses: bool = True\\n) -> str:\\n    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\\n    query_user, job_id_description = query_message(query, df, model=model, token_budget=token_budget)\\n    messages = [\\n        {\"role\": \"system\", \"content\": system_prompt},\\n        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\\n        {\"role\": \"assistant\", \"content\": job_id_description}\\n    ]\\n    if print_gpt_messages:\\n        print(messages)\\n    response = openai.ChatCompletion.create(\\n        model=model,\\n        messages=messages,\\n        temperature=0\\n    )\\n    response_message = response[\"choices\"][0][\"message\"][\"content\"]\\n    \\n    if print_cost_and_relatednesses:\\n        total_tokens = response[\\'usage\\'][\\'total_tokens\\']\\n        prompt_tokens = response[\\'usage\\'][\\'prompt_tokens\\']\\n        completion_tokens = response[\\'usage\\'][\\'completion_tokens\\']\\n        print(f\"\\nPROMPT TOKENS USED:{prompt_tokens}\\n\", f\"COMPLETION TOKENS USED:{completion_tokens}\\n\", f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\\n        #Approximate cost\\n        if GPT_MODEL == \"gpt-4\":\\n            prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\\n            completion_cost = round((completion_tokens / 1000) * 0.06, 3)\\n            cost_classify = prompt_cost + completion_cost\\n            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\\n            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\\n            #final_cost = total_cost_summarise_job + cost_classify\\n            #print(f\"FINAL COST: ${final_cost} USD\")\\n        elif GPT_MODEL == \"gpt-3.5-turbo\":\\n            prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\\n            completion_cost = round((completion_tokens / 1000) * 0.002, 3)\\n            cost_classify = prompt_cost + completion_cost\\n            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\\n            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\\n            #final_cost = total_cost_summarise_job + cost_classify\\n            #print(f\"FINAL COST: ${final_cost} USD\")\\n        elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\\n            prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\\n            completion_cost = round((completion_tokens / 1000) * 0.004, 3)\\n            cost_classify = prompt_cost + completion_cost\\n            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\\n            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\\n            #final_cost = total_cost_summarise_job + cost_classify\\n            #print(f\"FINAL COST: ${final_cost} USD\")\\n\\n        #relatednesses\\n        ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df)\\n        print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\\n        for id, relatedness in zip(ids, relatednesses):\\n            print(f\"ID: {id} has the following {relatedness=:.3f}\")\\n        \\n        elapsed_time = timeit.default_timer() - start_time\\n        print(\"\\n\", f\"DreamedJobAI finished! all in: {elapsed_time:.2f} seconds\", \"\\n\") \\n    return response_message'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 8192,\n",
    "    print_gpt_messages: bool = True,\n",
    "    print_cost_and_relatednesses: bool = True\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    query_user, job_id_description = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": f\"{delimiters}{query_user}{delimiters}\"},\n",
    "        {\"role\": \"assistant\", \"content\": job_id_description}\n",
    "    ]\n",
    "    if print_gpt_messages:\n",
    "        print(messages)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    \n",
    "    if print_cost_and_relatednesses:\n",
    "        total_tokens = response['usage']['total_tokens']\n",
    "        prompt_tokens = response['usage']['prompt_tokens']\n",
    "        completion_tokens = response['usage']['completion_tokens']\n",
    "        print(f\"\\nPROMPT TOKENS USED:{prompt_tokens}\\n\", f\"COMPLETION TOKENS USED:{completion_tokens}\\n\", f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\n",
    "        #Approximate cost\n",
    "        if GPT_MODEL == \"gpt-4\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.03, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.06, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            #final_cost = total_cost_summarise_job + cost_classify\n",
    "            #print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.0015, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.002, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            #final_cost = total_cost_summarise_job + cost_classify\n",
    "            #print(f\"FINAL COST: ${final_cost} USD\")\n",
    "        elif GPT_MODEL == \"gpt-3.5-turbo-16k\":\n",
    "            prompt_cost = round((prompt_tokens / 1000) * 0.003, 3)\n",
    "            completion_cost = round((completion_tokens / 1000) * 0.004, 3)\n",
    "            cost_classify = prompt_cost + completion_cost\n",
    "            print(f\"COST FOR CLASSIFYING:\", f\"${cost_classify} USD\")\n",
    "            #print(f\"COST FOR SUMMARISING:\", f\"${total_cost_summarise_job} USD\")\n",
    "            #final_cost = total_cost_summarise_job + cost_classify\n",
    "            #print(f\"FINAL COST: ${final_cost} USD\")\n",
    "\n",
    "        #relatednesses\n",
    "        ids, relatednesses = ids_ranked_by_relatedness_e5(query=query, df=df)\n",
    "        print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "        for id, relatedness in zip(ids, relatednesses):\n",
    "            print(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "        \n",
    "        elapsed_time = timeit.default_timer() - start_time\n",
    "        print(\"\\n\", f\"DreamedJobAI finished! all in: {elapsed_time:.2f} seconds\", \"\\n\") \n",
    "    return response_message\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ask(abstract_cv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(a):\n",
    "    if a == 0:\n",
    "        data = '[{\"id\": \"35204\", \"suitability\": \"Not Suitable\", \"explanation\": \"The candidate does not have the required experience in iOS development, leadership roles, or the specific technical skills listed in the job description.\"}]'\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loop number: 1\n",
      "[{'role': 'system', 'content': \"\\n\\nYou are a job recruiter for a large recruitment agency./\\nYou will be provided with a candidate's CV./\\nThe CV will be delimited with #### characters./\\nYou will also be provided with the Job IDs (delimited by angle brackets) /\\nand corresponding descriptions (delimited by triple dashes)/\\nfor the available job openings./\\n\\nPerform the following steps:/\\n\\nStep 1 - Classify the provided CV into a suitability category for each job opening./\\nStep 2 - For each ID briefly explain in one sentence your reasoning behind the chosen suitability category./\\nStep 3 - Only provide your output in json format with the keys: id, suitability and explanation./\\n\\nDo not classify a CV into a suitability category until you have classify the CV yourself.\\n\\nSuitability categories: Highly Suitable, Moderately Suitable,/\\nPotentially Suitable, Marginally Suitable and Not Suitable./\\n\\nHighly Suitable: CVs in this category closely align with the job opening, demonstrating extensive relevant experience, skills, and qualifications. The candidate possesses all or most of the necessary requirements and is an excellent fit for the role./\\nModerately Suitable: CVs falling into this category show a reasonable match to the job opening. The candidate possesses some relevant experience, skills, and qualifications that align with the role, but there may be minor gaps or areas for improvement. With some additional training or development, they could become an effective candidate./\\nPotentially Suitable: CVs in this category exhibit potential and may possess transferable skills or experience that could be valuable for the job opening. Although they may not meet all the specific requirements, their overall profile suggests that they could excel with the right support and training./\\nMarginally Suitable: CVs falling into this category show limited alignment with the job opening. The candidate possesses a few relevant skills or experience, but there are significant gaps or deficiencies in their qualifications. They may require substantial training or experience to meet the requirements of the role./\\nNot Suitable: CVs in this category do not match the requirements and qualifications of the job opening. The candidate lacks the necessary skills, experience, or qualifications, making them unsuitable for the role./\\n\"}, {'role': 'user', 'content': '####(\\'Qualifications: \\n- LLB Law degree from Universidad de las Américas Puebla (UDLAP) with an accumulated average of 9.4/10.\\n- Currently on an international exchange at the University of Bristol for the final year of studying Law.\\n- Member of the Honours Program at UDLAP, conducting research on FinTech, Financial Inclusion, Blockchain, Cryptocurrencies, and Smart Contracts.\\n\\nPrevious job titles:\\n- Data Analyst at Tata Consultancy Services México, where I cleansed, interpreted, and analyzed data using Python and SQL Server to produce visual reports with Power BI.\\n- Legal Assistant at BLACKSHIIP Venture Capital, responsible for proofreading and drafting legal documents, as well as assisting with negotiations of International Share Purchase Agreements.\\n\\nResponsibilities/Key Duties:\\n- Developed and introduced A/B testing to make data-driven business decisions as a Data Analyst Jr. at AMATL GRÁFICOS.\\n- Taught mental arithmetic as a Mathematics Instructor at ALOHA Mental Arithmetic.\\n- Led and assisted individuals with physical and mental disabilities as a Special Needs Counsellor at Camp Merrywood and YMCA Camp Independence.\\n\\nSkills:\\n- Proficient in Python, SQL Server, Tableau, Power BI, Bash/Command Line, Git & GitHub, and Office 365.\\n- Strong written and verbal communication skills, teamwork, ability to work under pressure, attention to detail, and leadership skills.\\n- Knowledge in machine learning, probabilities & statistics, and proofreading.\\n\\nOther Achievements:\\n- Published paper on \"Smart Legal Contracts: From Theory to Reality\" and participated in the IDEAS Summer Program on Intelligence, Data, Ethics, and Society at the University of California, San Diego.####'}, {'role': 'assistant', 'content': '\\n\\n\\n\\n Available job openings:\\n\\n\\n\\nID:<35204>\\nJob Description:---Job Information:\\n- Job Title: iOS Lead\\n- Job Objective: To secure the operation of mobile applications\\n- Responsibilities/Key Duties: \\n  - Coordinate with CTO and Software Architecture for technology and development processes\\n  - Ensure good development practices are followed\\n  - Coordinate staff assignments and conduct technical interviews\\n  - Conduct team performance evaluations and provide support\\n  - Make technical decisions and deal with customers\\n- Qualifications/Requirements: \\n  - More than five years of experience\\n  - Advanced English\\n  - Experience in leadership roles\\n  - Deep knowledge of Architectural patterns\\n  - Experience with UIKit, SwiftUI, CoreData, Combine, Mapkit, and Apple Watch development\\n  - Experience managing apps in the App Store\\n  - Hands-on experience in Unit Testing\\n- Preferred Skills/Experience: \\n  - Strong organizational skills\\n  - Curious, resourceful, and eager to tackle new challenges\\n  - Experience planning, designing, and implementing strategies for large-scale system software\\n  - Experience working with cross-functional teams in a fast-growing environment\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Umvel, part of NTT DATA\\n- Location: Mexico and the US\\n- Culture and Values: Committed to creating a more equitable world, no discrimination based on religion, age, gender, culture, or social status\\n- Compensation and Benefits: \\n  - Vacation after 6 months, vacation day by law, Christmas week off\\n  - Variable annual salary increase based on performance evaluation\\n  - English classes\\n  - Major medical expenses insurance\\n  - Medical discount card\\n  - Project participation bonus\\n  - Seniority bonus\\n  - Career growth path---\\n\\nID:<33815>\\nJob Description:---Job Information:\\n- Job Title: Analytics Engineering Lead\\n- Job Objective: The objective of the position is to serve as a data architect for Monzo\\'s Financial Crime and Fraud data, contributing to the design and scalability of data models that measure the effectiveness of fraud and financial crime controls.\\n- Responsibilities/Key Duties: The responsibilities include developing robust data models downstream of backend services, primarily in BigQuery, to support internal reporting, machine learning, and financial and regulatory use cases. The role also involves shaping and maintaining best practices for the Data Warehouse, collaborating with the Financial Crime and Fraud domain, and prioritizing data governance issues.\\n- Qualifications/Requirements: The ideal candidate should have a strong passion for data modeling, ETL projects, and Big Data, with experience as a developer or analyst. They should be comfortable working with data streams from various services and have expertise in SQL, data modeling, and general Data Warehousing concepts. Experience in building robust and reliable data sets is also required.\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Monzo\\n- Location: United Kingdom, with the option for distributed working within the UK and ad hoc meetings in London.\\n- Culture and Values: Monzo aims to make banking less complex and opaque, focusing on solving problems rather than selling financial products. They have an amazing community that suggests features, tests the app, and provides constant feedback.\\n- Compensation and Benefits: The job posting does not provide specific information about compensation and benefits.---\\n\\nID:<43812>\\nJob Description:---Job Information:\\n- Job Title: Financial Reporting Analyst\\n- Job Objective: The successful candidate will be responsible for preparing consolidated financial reporting, calculating and coordinating incentive settlements, developing an annual budget, monitoring progress, providing routine reporting, performing detailed analysis, creating standardized processes, and assisting in financial due diligence.\\n- Responsibilities/Key Duties: Prepare monthly consolidated financial reporting, analyze results, calculate incentive fees, establish reporting protocols, develop and monitor annual budgets, distribute monthly reports, maintain expenditure controls, support forecasting efforts, perform detailed analysis on performance, create executive summaries, assist in due diligence efforts, develop protocols and processes for data collection and financial reporting.\\n- Qualifications/Requirements: At least 2 years\\' experience in financial reporting consolidation, proficiency in Excel and PowerPoint, financial business acumen, knowledge of US partnerships, experience with process development and improvement, ability to prioritize tasks and work under time constraints, strong problem-solving and analytical skills, effective communication skills, ability to develop and maintain relationships with colleagues.\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Dentons, the world\\'s largest law firm\\n- Location: Not specified\\n- Culture, Values, and Mission: Dentons is known for delivering quality and value to clients globally. They challenge the status quo and advance client interests in the communities they operate in.\\n- Compensation and Benefits: Not mentioned in the job opening.---\\n\\nID:<35454>\\nJob Description:---Job Information:\\n- Job Title: Staff Software Engineer\\n- Job Objective: To lead a small team of backend and ML engineers in building highly scalable systems with micro-service architecture for new markets\\n- Responsibilities/Key Duties: Planning, designing, and building elegant solutions, managing the software development process, designing and implementing the overall architecture, ensuring speed and scalability, building REST APIs, implementing continuous integration and deployment, generating ideas for new initiatives and technologies, being a domain expert in the payment processing industry, communicating with stakeholders in various departments\\n- Qualifications/Requirements: Fluent in English, Bachelor\\'s Degree or higher in a quantitative field, 8+ years of experience in backend software development, 6+ years of professional experience with Python, expertise in AWS tech stack and Terraform or related technologies\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Galileo\\n- Location: Not specified\\n- Culture/Values/Mission: Welcoming, collaborative, and focused on making an impact\\n- Financial Aspects: Not specified\\n- Additional Perks: Not specified---\\n\\nID:<44479>\\nJob Description:---Job Information:\\n- Job Title: Contract Customer Data Analyst\\n- Job Objective: The objective of this position is to collaborate with internal stakeholders and interface with Subscribe customers to understand their needs, analyze their existing product catalog and historical order details, and import that data into Subscribe.\\n- Responsibilities/Key Duties: The responsibilities include working directly with customers to understand their product catalogs and order history, transforming data into a format that can be imported into Subskribe, filling in gaps and inaccuracies in customer data, and collaborating with the engineering team to define requirements.\\n- Qualifications/Requirements: The qualifications for this position include 2-5 years of experience with data entry or analysis, experience dealing with financial data, proficiency in using spreadsheets, excellent communication skills, ability to work with data import tools, and strong analytical skills.\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Subskribe Inc.\\n- Location: Worldwide Remote\\n- Company Culture/Values/Mission: Subskribe is a fast-moving, fast-growing company that is building the smartest, most scalable, and easiest-to-customize CPQ Subscription Billing and Revenue Recognition platform for SaaS companies. The company is committed to attracting, retaining, and developing a highly qualified, diverse, and dedicated team.\\n- Compensation and Benefits: The job is a 3-6 month contract with the potential to convert to full-time. The company is well-funded by top Silicon Valley investors.---\\n\\nID:<44194>\\nJob Description:---Job Information:\\n- Job Title: Business Analyst\\n- Job Objective: The Business Analyst will be responsible for evaluating business processes, identifying areas for improvement, and implementing solutions. They will also stay up-to-date on the latest process and IT advancements, conduct meetings and presentations, perform requirements analysis, and communicate insights and plans to cross-functional team members and management. Additionally, they will gather critical information, provide leadership training, allocate resources, ensure solutions meet business needs, and manage projects.\\n\\nQualifications/Requirements:\\n- Bachelor\\'s degree in business or related field or an MBA\\n- Minimum of 5 years of experience in business analysis or a related field\\n- Exceptional analytical and conceptual thinking skills\\n- Ability to influence stakeholders and work closely with them to determine acceptable solutions\\n- Advanced technical skills\\n- Excellent documentation skills\\n- Fundamental analytical and conceptual thinking skills\\n- Experience creating detailed reports and giving presentations\\n- Competency in Microsoft applications including Word, Excel, and Outlook\\n- Track record of following through on commitments\\n- Excellent planning, organizational, and time management skills\\n- Experience leading and developing top-performing teams\\n\\nAbout the company and Compensation and Benefits:\\n- Company Name: Nisum\\n- Location: California, with offices in the United States, Chile, Colombia, India, Pakistan, and Canada\\n- Culture and Values: Nisum is a customer-centric company with the motto \"Building Success Together.\" They enable clients to achieve direct business growth by building advanced technology solutions for immersive and seamless experiences across digital and physical channels.\\n- Compensation and Benefits: No information provided.---\\n\\nID:<35904>\\nJob Description:---Job Information:\\n- Job Title: Junior AML Analyst\\n- Job Objective: To support the AML Team in ensuring effective client and matter risk controls are in place and operate across the full range of practice areas within the firm globally. To contribute to ensuring that the firm complies with its regulatory obligations and appropriately manages and mitigates risk issues.\\n- Responsibilities/Key Duties: Conduct client due diligence (CDD) on new and existing clients and matters, undertake client and matter risk assessments, assist with ongoing monitoring of existing clients and matters, undertake sanction update screening, advise on matter opening processes, assist with internal audit processes, review and update internal compliance policies and procedures, assist with projects and ad hoc assignments.\\n\\nQualifications/Requirements:\\n- Relevant university degree\\n- First legal or compliance experience working in a law firm or professional services environment would be an asset\\n- Highly effective research skills\\n- Good knowledge of relevant compliance rules and regulations\\n- Strong problem-solving and analytical skills\\n- Excellent written and verbal communication skills\\n- Customer-facing skills\\n- Efficient and organized approach to administration\\n\\nPreferred Skills/Experience: Dynamic and flexible team players with highly developed critical thinking, ability to assess legal and commercial risks and advise on appropriate mitigation strategies.\\n\\nAbout the Company and Compensation and Benefits:\\n- Company Name: Dentons\\n- Location: Czechia\\n- Dentons is the world\\'s largest law firm delivering quality and value to clients.\\n- Additional perks and benefits not mentioned.---\\n\\nID:<44444>\\nJob Description:---Job Information:\\n- Job Title: Analyst Customer 360 Insights\\n- Job Objective: Improve and scale Upwork customer experience through actionable insights\\n- Responsibilities/Key Duties: \\n  - Conduct data analysis and develop dashboards\\n  - Develop and manage ETL processes\\n  - Perform advanced analytics to generate insights\\n  - Refine questions and generate hypotheses about the product and business\\n  - Create customer experience journey maps\\n  - Identify opportunities for automation and self-serve mechanisms\\n- Qualifications/Requirements: \\n  - Highly analytical and creative in turning large data into insights\\n  - Experience in data analysis and communicating insights\\n  - Expertise in SQL, ETL, statistical analysis, and data visualization tools\\n  - Strong grasp of statistical concepts and advanced analytics\\n  - Experience with self-serve reports and visualizations\\n  - Passion for using data and analytics to support business decisions\\n  - Prior experience in Analytics, Business Intelligence, or Data Science preferred\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Upwork\\n- Location: Global\\n- Culture and Values: Upwork is committed to fostering a diverse and inclusive workforce\\n- Compensation and Benefits: Not mentioned in the job opening.---\\n\\nID:<44446>\\nJob Description:---Job Information:\\n- Job Title: Staff Software Engineer\\n- Job Objective: The Staff Software Engineer will be responsible for architecting stream processing features, developing robust data pipelines, designing and building experimentation infrastructure, and improving the reliability and efficiency of core data processing systems. They will also work cross-functionally with various teams to identify and execute new opportunities in experimentation.\\n- Responsibilities/Key Duties: Architect stream processing features, develop data pipelines, design and build experimentation infrastructure, improve reliability and efficiency of data processing systems, work cross-functionally with other teams, provide statistical analysis and visualization tooling.\\n- Qualifications/Requirements: 8 years of industry experience in building large-scale production systems, experience with stream processing systems, solid understanding of databases, experience with data warehouse solutions, experience with statistical analysis tooling, experience leading technical projects, BS/MS/PhD in Computer Science or related field.\\n\\nAbout the company and Compensation and Benefits:\\n- Company: Affirm\\n- Location: Spain\\n- Culture/Values/Mission: Affirm is reinventing credit to make it more honest and friendly, providing consumers with the flexibility to buy now and pay later without hidden fees or compounding interest.\\n- Compensation and Benefits: Affirm offers a remotefirst work environment, with the majority of roles being remote. Some roles may require occasional work from an assigned office. The company has a transparent compensation structure based on job-related skills, experience, and education. Certain roles are eligible for equity awards based on performance and tenure milestones. Visa sponsorship is available for the role, but the candidate must be based in Spain.---\\n\\nID:<36697>\\nJob Description:---Job Information:\\n- Job Title: DBA Oracle Remoto MX\\n- Job Objective: The company is seeking a DBA Oracle with certifications in ITIL Foundation V3 or higher, Oracle Database Administrator V11g or higher, Oracle 12C Administrator or higher, and Oracle GoldenGate 10 or higher. The position requires expertise in staffing projects, headhunting, and recruitment of IT specialists.\\n- Responsibilities/Key Duties: The DBA Oracle will be responsible for managing Oracle databases, ensuring their performance, security, and availability. They will also be involved in project staffing, headhunting, and recruitment of IT specialists.\\n- Qualifications/Requirements: The candidate must have a valid certification in ITIL Foundation V3 or higher, Oracle Database Administrator V11g or higher, Oracle 12C Administrator or higher, and Oracle GoldenGate 10 or higher.\\n\\nPreferred Skills/Experience: Not mentioned in the provided passage.\\n\\nAbout the Company and Compensation and Benefits:\\n- Company: ITC Home\\n- Location: Remote/Anywhere in Mexico\\n- Culture/Values/Mission: Not mentioned in the provided passage.\\n- Compensation and Benefits: The company offers a competitive salary, payroll + legal benefits, and additional perks such as a savings account, discounts, preferential prices, major medical expenses insurance, life insurance, and the opportunity to work from home.---\\n'}]\n",
      "\n",
      "PROMPT TOKENS USED:3787\n",
      " COMPLETION TOKENS USED:587\n",
      " \n",
      "TOTAL TOKENS USED:4374\n",
      "\n",
      "COST FOR CLASSIFYING: $0.14900000000000002 USD\n",
      "COST FOR SUMMARISING: $0.032 USD\n",
      "FINAL COST: $0.18100000000000002 USD\n",
      "\n",
      "THE IDs ARE RANKED BY RELEVANCE:\n",
      "\n",
      "ID: 35204 has the following relatedness=0.870\n",
      "ID: 33815 has the following relatedness=0.868\n",
      "ID: 43812 has the following relatedness=0.865\n",
      "ID: 35454 has the following relatedness=0.862\n",
      "ID: 44479 has the following relatedness=0.860\n",
      "ID: 44194 has the following relatedness=0.860\n",
      "ID: 35904 has the following relatedness=0.859\n",
      "ID: 44444 has the following relatedness=0.859\n",
      "ID: 44446 has the following relatedness=0.859\n",
      "ID: 36697 has the following relatedness=0.858\n",
      "\n",
      " DreamedJobAI finished! all in: 124.00 seconds \n",
      "\n",
      "<class 'list'> [{'id': '35204', 'suitability': 'Not Suitable', 'explanation': \"The candidate's CV does not show any experience or skills related to iOS development, leadership roles, or the specific technologies mentioned in the job description.\"}, {'id': '33815', 'suitability': 'Highly Suitable', 'explanation': 'The candidate has relevant experience as a Data Analyst, is proficient in Python and SQL Server, and has knowledge in machine learning and statistics, which aligns well with the job requirements.'}, {'id': '43812', 'suitability': 'Potentially Suitable', 'explanation': 'The candidate has experience as a Data Analyst and Legal Assistant, which could be valuable for the financial reporting role, but lacks specific experience in financial reporting consolidation.'}, {'id': '35454', 'suitability': 'Moderately Suitable', 'explanation': 'The candidate has experience in Python and data analysis, which are relevant to the role, but lacks specific experience in backend software development and AWS tech stack.'}, {'id': '44479', 'suitability': 'Highly Suitable', 'explanation': \"The candidate's experience as a Data Analyst, proficiency in Python and SQL Server, and experience dealing with financial data align well with the job requirements.\"}, {'id': '44194', 'suitability': 'Potentially Suitable', 'explanation': 'The candidate has experience in data analysis and has demonstrated leadership skills, which could be valuable for the Business Analyst role, but lacks specific experience in business analysis.'}, {'id': '35904', 'suitability': 'Moderately Suitable', 'explanation': 'The candidate has a law degree and experience working in a law firm, which are relevant to the AML Analyst role, but lacks specific experience in compliance or AML.'}, {'id': '44444', 'suitability': 'Highly Suitable', 'explanation': \"The candidate's experience as a Data Analyst, proficiency in Python and SQL Server, and knowledge in machine learning and statistics align well with the job requirements.\"}, {'id': '44446', 'suitability': 'Potentially Suitable', 'explanation': 'The candidate has experience in Python and data analysis, which are relevant to the role, but lacks specific experience in building large-scale production systems and stream processing systems.'}, {'id': '36697', 'suitability': 'Not Suitable', 'explanation': \"The candidate's CV does not show any experience or skills related to Oracle databases or the specific certifications mentioned in the job description.\"}]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def check_output_GPT4(function_gpt4: Callable, input_cv: str) -> str:\n",
    "    default = '[{\"id\": \"\", \"suitability\": \"\", \"explanation\": \"\"}]'\n",
    "    default_json = json.loads(default)\n",
    "    \n",
    "    for _ in range(4):\n",
    "        i = _ + 1\n",
    "        print(f\"Loop number: {i}\")\n",
    "        try:\n",
    "            python_string = function_gpt4(input_cv)\n",
    "            try:\n",
    "                data = json.loads(python_string)\n",
    "                return data\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        except OpenAIError as e:\n",
    "            print(f\"{e}. Retrying in 10 seconds. Number of retries: {i}\")\n",
    "            time.sleep(10)\n",
    "            pass\n",
    "        except Exception as e:\n",
    "            print(f\"{e}. Retrying in 5 seconds. Number of retries: {i}\")\n",
    "            time.sleep(5)\n",
    "            pass\n",
    "\n",
    "    \n",
    "    print(\"Check logs!!!! Main function was not callable\")\n",
    "    return default_json\n",
    "\n",
    "checked_json = check_output_GPT4(ask, abstract_cv)\n",
    "#exp = check_output_GPT4(get_data, 0)\n",
    "\n",
    "print(type(checked_json), checked_json)\n",
    "#print(type(exp), exp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> ('35204', '33815', '43812', '35454', '44479', '44194', '35904', '44444', '44446', '36697')\n"
     ]
    }
   ],
   "source": [
    "def ids_json_loads(data: list[dict[str, str, str]] = None) -> str:\n",
    "    if data is None:\n",
    "        data = checked_json\n",
    "    \n",
    "    ids = \"\"\n",
    "    for item in data:\n",
    "        if \"id\" in item:\n",
    "            if ids:\n",
    "                ids += \", \"\n",
    "            ids += f\"'{item['id']}'\"\n",
    "\n",
    "    return f\"({ids})\"\n",
    "\n",
    "ids_ready = ids_json_loads()\n",
    "print(type(ids_ready), ids_ready)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> [(43812, 'Financial Reporting Analyst (US Region)', 'https://himalayas.app/companies/dentons/jobs/financial-reporting-analyst-us-region-8736571922', 'CR'), (44194, 'Business Analyst A4897', 'https://tryremotely.com/remote-jobs/nisum-business-analyst-a4897-27151', 'Chile'), (44444, 'Contract: Analyst, Customer 360 Insights', 'https://tryremotely.com/remote-jobs/upwork-contract-analyst-customer-360-insights-26657', 'South America'), (44446, 'Staff Software Engineer (Experimentation)', 'https://tryremotely.com/remote-jobs/affirm-staff-software-engineer-experimentation-26655', 'Spain'), (44479, 'Contract Customer Data Analyst', 'https://tryremotely.com/remote-jobs/subskribe-contract-customer-data-analyst-26614', 'Worldwide')]\n"
     ]
    }
   ],
   "source": [
    "def join_postgre_data_with_ids(ids:str) -> list :\n",
    "    conn = psycopg2.connect(user=user, password=password, host=host, port=port, database=database)\n",
    "\n",
    "    # Create a cursor object\n",
    "    cur = conn.cursor()\n",
    "    cur.execute( f\"SELECT id, title, link, location FROM recent_jobs WHERE id IN {ids}\")\n",
    "\n",
    "    # Fetch all rows from the table\n",
    "    rows = cur.fetchall()\n",
    "\n",
    "    # Close the database connection\n",
    "    cur.close()\n",
    "    conn.close()\n",
    "\n",
    "    return rows\n",
    "\n",
    "ids_postgre_data = join_postgre_data_with_ids(ids=ids_ready)\n",
    "\n",
    "print(type(ids_postgre_data), ids_postgre_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(43812, 'Financial Reporting Analyst (US Region)', 'https://himalayas.app/companies/dentons/jobs/financial-reporting-analyst-us-region-8736571922', 'CR', 'Potentially Suitable', 'The candidate has experience as a Data Analyst and Legal Assistant, which could be valuable for the financial reporting role, but lacks specific experience in financial reporting consolidation.'), (44194, 'Business Analyst A4897', 'https://tryremotely.com/remote-jobs/nisum-business-analyst-a4897-27151', 'Chile', 'Potentially Suitable', 'The candidate has experience in data analysis and has demonstrated leadership skills, which could be valuable for the Business Analyst role, but lacks specific experience in business analysis.'), (44444, 'Contract: Analyst, Customer 360 Insights', 'https://tryremotely.com/remote-jobs/upwork-contract-analyst-customer-360-insights-26657', 'South America', 'Highly Suitable', \"The candidate's experience as a Data Analyst, proficiency in Python and SQL Server, and knowledge in machine learning and statistics align well with the job requirements.\"), (44446, 'Staff Software Engineer (Experimentation)', 'https://tryremotely.com/remote-jobs/affirm-staff-software-engineer-experimentation-26655', 'Spain', 'Potentially Suitable', 'The candidate has experience in Python and data analysis, which are relevant to the role, but lacks specific experience in building large-scale production systems and stream processing systems.'), (44479, 'Contract Customer Data Analyst', 'https://tryremotely.com/remote-jobs/subskribe-contract-customer-data-analyst-26614', 'Worldwide', 'Highly Suitable', \"The candidate's experience as a Data Analyst, proficiency in Python and SQL Server, and experience dealing with financial data align well with the job requirements.\")]\n"
     ]
    }
   ],
   "source": [
    "def adding_all_data(joined_data: list, suitable_jobs: list) -> list:\n",
    "    for i, item in enumerate(joined_data):\n",
    "        entry_id = item[0]  # Extract the ID from the tuple\n",
    "        for json_item in suitable_jobs:\n",
    "            if int(json_item['id']) == entry_id:\n",
    "                suitability = json_item['suitability']\n",
    "                explanation = json_item['explanation']\n",
    "                joined_data[i] = item + (suitability, explanation)\n",
    "                break\n",
    "    return joined_data\n",
    "\n",
    "updated_data = adding_all_data(ids_postgre_data, checked_json)\n",
    "\n",
    "print(updated_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(43812, 'Financial Reporting Analyst (US Region)', 'https://himalayas.app/companies/dentons/jobs/financial-reporting-analyst-us-region-8736571922', 'CR', 'Potentially Suitable', 'The candidate has experience as a Data Analyst and Legal Assistant, which could be valuable for the financial reporting role, but lacks specific experience in financial reporting consolidation.')\n"
     ]
    }
   ],
   "source": [
    "j = updated_data[0]\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_3_summary = \"\"\"Write an abstract description of the following CV in four bullet points. \n",
    "\n",
    "Let's think step by step to summarise it into four bullet points:\n",
    "\n",
    "1. Focus on the main skills and responsibilities of each role. \n",
    "2. One of the bullet points is the total years of experience.\n",
    "2. Omit the employer names. \n",
    "3. Your answer must be in an active voice. \n",
    "4. Double-check that the summary is in four bullet points, three summarising the main skills and responsibilities and the last one is about the years of experience. \n",
    "CV IS BELOW: \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "E5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
