{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken  # for counting tokens\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "from scipy import spatial\n",
    "import pretty_errors\n",
    "from preprocess import individual_preprocess\n",
    "from displayfunction import display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       [0.0011792784789577127, -0.0062871999107301235...\n",
      "1       [0.00448582973331213, -0.009026281535625458, -...\n",
      "2       [0.0032345347572118044, -0.008394144475460052,...\n",
      "3       [0.0010413312120363116, -0.0062429034151136875...\n",
      "4       [0.008784311823546886, -0.005815159995108843, ...\n",
      "                              ...                        \n",
      "1738    [-0.026897858828306198, 0.006618379149585962, ...\n",
      "1739    [-0.027289917692542076, -0.014915977604687214,...\n",
      "1740    [-0.009569555521011353, -0.0051603857427835464...\n",
      "1741    [-0.025336530059576035, -0.012708911672234535,...\n",
      "1742    [-0.02954697422683239, -0.026450172066688538, ...\n",
      "Name: embeddings, Length: 1743, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "load_dotenv('.env')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "SAVE_PATH = os.getenv(\"SAVE_PATH\")\n",
    "E5_BASE_TOTAL_JOBS = os.getenv(\"E5_BASE_TOTAL_JOBS\")\n",
    "OPENAI_TOTAL_JOBS = os.getenv(\"OPENAI_TOTAL_JOBS\")\n",
    "\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "#GPT_MODEL = \"gpt-4\"\n",
    "\n",
    "\"\"\"\n",
    "Load the embedded file\n",
    "\"\"\"\n",
    "\n",
    "embeddings_path = OPENAI_TOTAL_JOBS\n",
    "\n",
    "df = pd.read_parquet(embeddings_path)\n",
    "\n",
    "# convert embeddings from CSV str type back to list type\n",
    "x = df[\"embeddings\"]\n",
    "print(x)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 2. Search\n",
    "\n",
    "Now we'll define a search function that:\n",
    "- Takes a user query and a dataframe with text & embedding columns\n",
    "- Embeds the user query with the OpenAI API\n",
    "- Uses distance between query embedding and text embeddings to rank the texts\n",
    "- Returns two lists:\n",
    "    - The top N texts, ranked by relevance\n",
    "    - Their corresponding relevance scores\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# search function\n",
    "def ids_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    #Modify this to get more jobs\n",
    "    top_n: int = 15\n",
    ") -> tuple[list[str], list[float]]:\n",
    "\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    \n",
    "    query_preprocess = individual_preprocess(query)\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query_preprocess,\n",
    "    )\n",
    "    #This is the query (e.g., Data Engineer) that the model will find relatednessnes\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    ids_and_relatednesses = [\n",
    "        (row[\"ids\"], relatedness_fn(query_embedding, row[\"embeddings\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    ids_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    ids, relatednesses = zip(*ids_and_relatednesses)\n",
    "    return ids[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "THE IDs ARE RANKED BY RELEVANCE:\n",
      "\n",
      "ID: 34207 has the following relatedness=0.837\n",
      "ID: 35211 has the following relatedness=0.834\n",
      "ID: 36738 has the following relatedness=0.830\n",
      "ID: 33752 has the following relatedness=0.830\n",
      "ID: 35222 has the following relatedness=0.829\n",
      "ID: 34204 has the following relatedness=0.828\n",
      "ID: 34294 has the following relatedness=0.826\n",
      "ID: 34155 has the following relatedness=0.826\n",
      "ID: 34191 has the following relatedness=0.824\n",
      "ID: 35209 has the following relatedness=0.823\n",
      "ID: 34244 has the following relatedness=0.823\n",
      "ID: 40019 has the following relatedness=0.822\n",
      "ID: 34186 has the following relatedness=0.820\n",
      "ID: 34237 has the following relatedness=0.818\n",
      "ID: 38202 has the following relatedness=0.818\n"
     ]
    }
   ],
   "source": [
    "query = \"query: Python Remote Worldwide or MX (Mexico)\"\n",
    "ids, relatednesses = ids_ranked_by_relatedness(query, df)\n",
    "print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "for id, relatedness in zip(ids, relatednesses):\n",
    "    print(f\"ID: {id} has the following {relatedness=:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiktoken function -> to count tokens\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_prompt = \"\"\"\n",
    "\n",
    "To assist you, here are the job IDs and the respective job descriptions ranked by relatedness to the user's specifications.\n",
    "Review these jobs and output the ID(s) of the job(s) that meet the following criteria:\n",
    "1. The job(s) contain the user's requirements; or\n",
    "2. The job(s) contain both \"requirements\" and \"preferences\". In this case, called them: \"Dream Jobs\"\n",
    "\n",
    "If no job meets all “requirements”, respond with \"I am sorry, I do not have your dream job just yet.\"\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    ids, relatednesses = ids_ranked_by_relatedness(query, df)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = introduction_prompt\n",
    "    question = f\"\\n\\Requirements and Preferences of the user: {query}\"\n",
    "    message = introduction\n",
    "    for id in ids:\n",
    "        #Get the text for GPT to answer the question\n",
    "        job_description = df[df['ids'] == id]['text_data'].values[0] \n",
    "        \n",
    "        # Add job descriptions to the message along with job ID\n",
    "        next_id = f'\\n\\nJob\\'s ID:\\n\"\"\"\\n{id}\\n\"\"\"\\nJob Description:\\n\"\"\"\\n{job_description}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_id + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return message + question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "\n",
    "You are DreamJobAI, an expert in job searching. \n",
    "Users will share the “requirements” and “preferences” of their dream job.\n",
    "You are going to be provided with jobs ranked by relatedness to the user's “requirements” and “preferences”, use those jobs to complete your task.\n",
    "*Your task is to find jobs which either meet the user's \"requirements\" or the user's \"requirements\" and “preferences”.* \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reminder = \"\"\"\n",
    "\n",
    "*Your task is to find jobs which either meet the user's \"requirements\" or the user's \"requirements\" and “preferences”.* \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_example_1= \"\"\"\n",
    "\n",
    "REQUIREMENTS: A job with Python. PREFERENCES: Data Engineer, Machine Learning \"\"\"\n",
    "\n",
    "assistant_example_1= \"\"\" \n",
    "\n",
    "These jobs meet your requirements, because they require someone fluent in Python:\n",
    "\n",
    "Job's ID: 1\n",
    "Job's ID: 2\n",
    "Job's ID: 3\n",
    "Job's ID: 4\n",
    "Job's ID: 5\n",
    "\n",
    "I found your Dream Job! It requires Python (requirement) and it is in Machine Learning (preferences):\n",
    "\n",
    "Job's ID: 3\n",
    "\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096,\n",
    "    print_messages: bool = True,\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"system\", \"name\":\"example_user\", \"content\": user_example_1},\n",
    "        {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": assistant_example_1},\n",
    "        {\"role\": \"user\", \"content\": user_reminder}\n",
    "    ]\n",
    "    if print_messages:\n",
    "        print(messages)\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    #Tokens\n",
    "    total_tokens = response['usage']['total_tokens']\n",
    "    print(f\"\\nTOTAL TOKENS USED:{total_tokens}\\n\", )\n",
    "    #Approximate cost\n",
    "    if GPT_MODEL == \"gpt-4\":\n",
    "        approximate_cost = round((total_tokens / 1000) * 0.045, 3)\n",
    "        print(f\"APPROXIMATE COST FOR QUERY:\", f\"${approximate_cost} USD\")\n",
    "    elif GPT_MODEL == \"gpt-3.5-turbo\":\n",
    "        approximate_cost = round((total_tokens / 1000) * 0.002, 3)\n",
    "        print(f\"APPROXIMATE COST FOR QUERY:\", f\"${approximate_cost} USD\")\n",
    "    \n",
    "    #relatednesses\n",
    "    ids, relatednesses = ids_ranked_by_relatedness(query=query, df=df)\n",
    "    print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "    for id, relatedness in zip(ids, relatednesses):\n",
    "        print(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask('REQUIREMENTS: Python Remote Worldwide or MX (Mexico). PREFERENCES: Data Engineer or Machine Learning'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(ask('REQUIREMENTS: Python developer. PREFERENCES: Machine Learning or Data Engineer'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
