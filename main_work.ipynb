{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "import pandas as pd\n",
    "import tiktoken  # for counting tokens\n",
    "import ast  # for converting embeddings saved as strings back to arrays\n",
    "from scipy import spatial\n",
    "import pretty_errors\n",
    "from preprocess import individual_preprocess\n",
    "from displayfunction import display\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv('.env')\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "SAVE_PATH = os.getenv(\"SAVE_PATH\")\n",
    "\n",
    "# models\n",
    "EMBEDDING_MODEL = \"text-embedding-ada-002\"\n",
    "GPT_MODEL = \"gpt-3.5-turbo\"\n",
    "#GPT_MODEL = \"gpt-4\"\n",
    "\n",
    "\"\"\"\n",
    "Load the embedded file\n",
    "\"\"\"\n",
    "\n",
    "embeddings_path = SAVE_PATH + \"/jobs_test5.csv\"\n",
    "\n",
    "df = pd.read_csv(embeddings_path)\n",
    "\n",
    "# convert embeddings from CSV str type back to list type\n",
    "df['embedding'] = df['embedding'].apply(ast.literal_eval)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 2. Search\n",
    "\n",
    "Now we'll define a search function that:\n",
    "- Takes a user query and a dataframe with text & embedding columns\n",
    "- Embeds the user query with the OpenAI API\n",
    "- Uses distance between query embedding and text embeddings to rank the texts\n",
    "- Returns two lists:\n",
    "    - The top N texts, ranked by relevance\n",
    "    - Their corresponding relevance scores\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# search function\n",
    "def ids_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y),\n",
    "    #Modify this to get more jobs\n",
    "    top_n: int = 20\n",
    ") -> tuple[list[str], list[float]]:\n",
    "\n",
    "    \"\"\"Returns a list of strings and relatednesses, sorted from most related to least.\"\"\"\n",
    "    \n",
    "    query_preprocess = individual_preprocess(query)\n",
    "    print(query_preprocess)\n",
    "    query_embedding_response = openai.Embedding.create(\n",
    "        model=EMBEDDING_MODEL,\n",
    "        input=query_preprocess,\n",
    "    )\n",
    "    #This is the query (e.g., Data Engineer) that the model will find relatednessnes\n",
    "    query_embedding = query_embedding_response[\"data\"][0][\"embedding\"]\n",
    "    ids_and_relatednesses = [\n",
    "        (row[\"id\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    ids_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    ids, relatednesses = zip(*ids_and_relatednesses)\n",
    "    return ids[:top_n], relatednesses[:top_n]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job good python developer three year experience would like something related machine learning data engineering\n",
      "(('2rPkshzK', 's9Qra2aO', 'lnstGfwL', 'QjJFvjn6', 'OFkXaI5H', '7pzvYS6y', '3qfbh5tM', 'gZaZ7bVt', '6A7XWtEj', 'DTwBPWYC', 'odXP3QLQ', 'wmvenYiI', 'GNl5gHGV', '6hlGa5dz', 'EPHJArCf', 'hHycd9Hz', 'Mb0O4TVn', 'nVHCI9OO', 'Y1y3y2fY', '0uvNXfS0'), (0.8447327505625101, 0.8384938610487017, 0.8363065683593817, 0.8356716883209322, 0.832334516584875, 0.8321505784975677, 0.8234213614182629, 0.8208861703149908, 0.8185780231693931, 0.8183886419248051, 0.8183886419248051, 0.8183886419248051, 0.813835545267674, 0.812246906023451, 0.8114809437229167, 0.810214728049277, 0.8091536896949553, 0.8091536896949553, 0.8091536896949553, 0.8091536896949553))\n"
     ]
    }
   ],
   "source": [
    "query = \"Which jobs are good for a Python developer with three years of experience? I would like something related with Machine Learning or Data Engineering\"\n",
    "\n",
    "print(ids_ranked_by_relatedness(query, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tiktoken function -> to count tokens\n",
    "def num_tokens(text: str, model: str = GPT_MODEL) -> int:\n",
    "    \"\"\"Return the number of tokens in a string.\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction = \"\"\"\n",
    "\n",
    "To assist you, here are job IDs ranked by relatedness to the users specifications.\n",
    "Review these jobs and identify 5-10 that may include the user's preferences and must meet their requirements.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_message(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    model: str,\n",
    "    token_budget: int\n",
    ") -> str:\n",
    "    \"\"\"Return a message for GPT, with relevant source texts pulled from a dataframe.\"\"\"\n",
    "    ids, relatednesses = ids_ranked_by_relatedness(query, df)\n",
    "    #Basically giving the most relevant IDs from the previous function\n",
    "    introduction = 'These are all the IDs of all the available jobs. Use them to find the jobs that match the skills of the user. If you cannot find a single job that might match then say \"I could not find a suitable job.\"'\n",
    "    question = f\"\\n\\Skills of the user: {query}\"\n",
    "    message = introduction\n",
    "    for id in ids:\n",
    "        next_id = f'\\n\\nJob\\'s ID:\\n\"\"\"\\n{id}\\n\"\"\"'\n",
    "        if (\n",
    "            num_tokens(message + next_id + question, model=model)\n",
    "            > token_budget\n",
    "        ):\n",
    "            break\n",
    "        else:\n",
    "            message += next_id\n",
    "    return message + question\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt=\"\"\"\n",
    "\n",
    "You are DreamJobAI, an expert in job searching. \n",
    "Users will share their dream job “preferences” and “requirements”. \n",
    "Your goal is to find jobs in your database that closely match their specifications. \n",
    "Jobs you suggest may not have all “preferences” but must meet all “requirements”. \n",
    "If no job meets all “requirements”, respond with \"I am sorry, I do not have your dream job just yet.\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assistant_prompt= \"\"\"\n",
    "Your output must be a PostgreSQL query. For example: \n",
    "\n",
    "SELECT * FROM personal WHERE ID IN ('vcRRzC7K', '8JNjW9zL', '3KmzR6pF', '5GmzR6pF', '9JNjW9zL', '2KmzR6pF', '6GmzR6pF')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(\n",
    "    #This query is your question, only parameter to fill in function\n",
    "    query: str,\n",
    "    df: pd.DataFrame = df,\n",
    "    model: str = GPT_MODEL,\n",
    "    token_budget: int = 4096,\n",
    "    print_message: bool = False,\n",
    ") -> str:\n",
    "    #Answers a query using GPT and a dataframe of relevant texts and embeddings.\n",
    "    message = query_message(query, df, model=model, token_budget=token_budget)\n",
    "    if print_message:\n",
    "        print(message)\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": message},\n",
    "        {\"role\": \"assistant\", \"content\": assistant_prompt}\n",
    "    ]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0.2\n",
    "    )\n",
    "    response_message = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "    #Tokens\n",
    "    print(\"'\\nTOTAL TOKENS USED:\\n'\", response['usage']['total_tokens'])\n",
    "    \n",
    "    #relatednesses\n",
    "    ids, relatednesses = ids_ranked_by_relatedness(query=query, df=df)\n",
    "    print(f\"\\nTHE IDs ARE RANKED BY RELEVANCE:\\n\")\n",
    "    for id, relatedness in zip(ids, relatednesses):\n",
    "        print(f\"ID: {id} has the following {relatedness=:.3f}\")\n",
    "    return response_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask('Which jobs are good for a Python developer with three years of experience? I would like something related with Machine Learning or Data Engineering'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ask('three years of professional experience. Fluent in Python, SQL Server, PostgreSQL, MySQL, Tableau, Git, Github y PowerBI. Puesto deseado: Software Engineer, Data Scientist, Data Engineer, Data Analyst'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
